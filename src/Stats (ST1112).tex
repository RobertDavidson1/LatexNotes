\documentclass[9pt]{extarticle}
\input{preamble.sty}

\input{macros.sty}

\usepackage[svgnames]{xcolor}
\usepackage{listings}


\title{
Robert Davidson \\
\textbf{ST1112: Statistics}
}
\author{
70\% Exam\\
30\% Continuous Assessment (3 parts)
}
\date{}       % Optional: Add date if desired
%--------------------------------------------------------
% Document
%--------------------------------------------------------
\begin{document}
\maketitle
\pagebreak

\tableofcontents
\pagebreak

\section{Inferential Statistics - Interval Estimation}
The ultimate goal in statistical inference is to estimate population parameters (like the mean $\mu$) based on sample statistics (like the sample mean $\bar{X}$).
\subsection{Probability vs Statistics}
\begin{itemize}
    \item \textbf{Probability} deals with known underlying processes: one starts with a model (like proportion of red vs. green jelly beans in a jar) and computes probability of specific outcomes
    \item \textbf{Statistics} works in reverse: one observes outcomes (sample data) and attempts to infer the underlying process or population parameters (e.g. proportion of red jellybeans)
\end{itemize}
\subsection{Definitions and Concepts}

\begin{definitionbox}{Population}{}
    A \textbf{population} is the complete set of items (or individuals) of interest.
\end{definitionbox}

\begin{definitionbox}{Sample}{}
    A \textbf{sample} is a subset of that population, intended to represent the population\\

    For example the sample mean $\bar{X}$ is an estimate of the population mean $\mu$.
\end{definitionbox}

\begin{definitionbox}{Population Mean ($\mu$)}{}
    $\mu$ represents the central tendency of a population distribution.
    $$\mu = \frac{1}{N} \sum_{i=1}^{N} x_i$$
    where $N$ is the population size and $x_i$ are the individual values in the population.\\

    $\mu$ is sometimes called the expected value or average.
\end{definitionbox}

\begin{definitionbox}{Population standard deviation ($\sigma$)}{}
    $\sigma$ measures the dispersion or spread of values around the mean in a population.
    $$\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2}$$
    where $N$ is the population size and $x_i$ are the individual values in the population.\\
\end{definitionbox}

\begin{conceptbox}{Sampling Variation}{}
    When we take multiple samples from the same population, each sample's mean $\bar{X}$ will be different. This is variability is called \textbf{sampling variation}. \\

    Larger sample sizes tend to reduce this variation, that is as $n$ gros, the sample mean $\bar{X}$ becomes a better estimate of the population mean $\mu$.
\end{conceptbox}
\begin{conceptbox}{Sampling Distributions}{}
    The sample mean itself is a \textbf{random variable} because different samples yield different mean values.\\

    The distribution of all possible sample means (of a given sample size $n$) is called the \textbf{sampling distribution} of the sample mean ($\bar{X}$).
\end{conceptbox}

\begin{definitionbox}{Expected Value of the Sample Mean}{}
    $$E(\bar{X}) = \mu$$
    This means if you averaged all possible sample means, you would get the population mean $\mu$.
\end{definitionbox}
\begin{definitionbox}{Standard Error of the Mean}{}
    $$SE = SD(\bar{X}) = \frac{\sigma}{\sqrt{n}}$$
    where $\sigma$ is the population standard deviation and $n$ is the sample size.\\

    This value is called the \textbf{standard error} of the mean and measures how much the sample mean $\bar{X}$ fluctuates around the population mean $\mu$.
\end{definitionbox}

\begin{definitionbox}{Central Limit Theorem}{}
    $$\bar{X} \sim N\left(\mu, \frac{\sigma^m}{n}\right)$$
    where $\bar{X}$ is the sample mean, $\mu$ is the population mean, and $\sigma$ is the population standard deviation.\\

    The \textbf{Central Limit Theorem} states that the sampling distribution of the sample mean $\bar{X}$ (the distribution of all sample means)  approaches a normal distribution as the sample size $n$ increases, \textbf{regardless of the shape of the population distribution.}\\

    This means that for large enough sample sizes, we can use the normal distribution to make inferences about the population mean $\mu$. \\

    \textbf{Practically}, many apply the rule of thumb $n \geq 30$ to treat $\bar{X}$ as normally distributed.
\end{definitionbox}


\begin{definitionbox}{Unbiased Estimators}{}
    We say a statistic $T$ is an \textbf{unbiased estimator} of a population parameter $\theta$,  if $E(T) = \theta$.\\

    For example, the sample mean $\bar{X}$ is an unbiased estimator of the population mean $\mu$ because $E(\bar{X}) = \mu$.\\

    The sample standard deviation $s$ (using Bessel's correction, dividing by multiplying by $\frac{1}{n-1}$ rather than $\frac{1}{N}$) is an unbiased estimator of the population standard deviation $\sigma$.
\end{definitionbox}

\subsection{Example}
\begin{examplebox}{Weekly rent}{}
    If a population mean rent is $\mu = 225$, with $\sigma = 25$ for a population sample size $n = 30$, the sample distribution of the sample mean is approximately:
    $$\bar{X} \sim N\left(225, \frac{25^2}{30}\right)$$
    This lets us compute probabilities for specific sample mean ranges using the normal distribution (e.g. $P(\bar{X} < 220)$).
\end{examplebox}
\subsection{Recap}
A \textbf{sample statistic} (e.g. the sample mean $\bar{X}$) varies from one sample to another. Understanding this variation (and quantifying it via the standard error) is crucial for knowing how precise (or imprecise) an estimate really is. \\[2ex]
If we have a large sample size $n$ from a population with mean $\mu$ and standard deviation $\sigma$, then our sample distribution of the sample mean $\bar{X}$ is approximately normal:
$$\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)$$
In practice, for $n \geq 30$, $\bar{X}$ can be treated as normally distributed even if the original population is not strictly normal.
\subsection{Confidence Intervals}
\begin{conceptbox}{Why confidence intervals?}{}
    Why do we need confidence intervals, instead of a single point estimate, like the sample mean $\bar{X}$? \\

    A confidence interval provides a range of plausible values for the population parameter (e.g. $\mu$) based on the sample data. \\

    \textbf{Analogy:}  Using a single point estimate is like trying to catch a fish wih a spear; your aim may not be perfect. Using a confidence interval is like using a net; we have a better chance of "catching" (capturing) the true population parameter.
\end{conceptbox}
\begin{definitionbox}{Confidence Interval}{}
    $$\bar{X} \pm (\text{critical value}) \times SE(\bar{X})$$
    where $SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}$ is the standard error of the sample mean, $\pm$ is the margin of error. \\
    The general formula for a desired confidence level 100(1- $\alpha$)\% is:
    $$\bar{X} \pm z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}}$$
    where $z_{\alpha/2}$ is the critical value from the standard normal distribution.
\end{definitionbox}
\noindent\textbf{Interpretation}: \\
If we repeat the sampling process many times and construct confidence intervals from each sample, then approximately $100 \times (1 - \alpha)\%$ of those intervals will contain the true population parameter $\mu$.\\
In other words, you do not say \emph{"there is a 95\% chance that $\mu$ lies in my interval"}. Rather we say, \textbf{"on repeated sampling 95\% of such intervals will contain the true population mean $\mu$."}


\begin{definitionbox}{Critical Values}{}
    The \textbf{critical value} is a z-score that corresponds to the desired confidence level. \\[2ex]
    For example, for a 95\% confidence level, the critical value is $Z_{\alpha/2} = 1.96$ (where $\alpha = 0.05$).
    This means that 95\% of the area under the normal curve lies within $1.96$ standard deviations of the mean. \\[2ex]
    \begin{center}
        \begin{tikzpicture}[
                domain=-1:1,
                xscale=-0.5,
                yscale=3.5,
                smooth,
                line cap=round,
                line join=round,
            ]
            % Define critical z values
            \def\zcrit{1.96}

            % Fill left tail area (2.5%)
            \fill[custom_red] (-3.5,0) -- plot[domain=-3.5:-\zcrit] (\x, {0.4*exp(-(\x)^2/2)}) -- (-\zcrit,0) -- cycle;

            % Fill right tail area (2.5%)
            \fill[custom_red] (3.5,0) -- plot[domain=3.5:\zcrit] (\x, {0.4*exp(-(\x)^2/2)}) -- (\zcrit,0) -- cycle;

            % Draw the normal distribution curve
            \draw[thick, custom_red] plot[domain=-3.5:3.5] (\x, {0.4*exp(-(\x)^2/2)});

            % Mark the critical values
            \draw[thick] (-\zcrit,0) -- (-\zcrit,0.05);
            \node[below] at (-\zcrit,0) {$-z_{0.025}$};
            \draw[thick] (\zcrit,0) -- (\zcrit,0.05);
            \node[below] at (\zcrit,0) {$z_{0.025}$};

            \draw[<->] (-\zcrit, -0.15) -- (\zcrit, -0.15) node[midway, below] {$95\%$};
            \draw[<->] (-\zcrit, -0.15) -- (-3.7, -0.15) node[midway, below] {$2.5\%$};
            \draw[<->] (\zcrit, -0.15) -- (3.7, -0.15) node[midway, below] {$2.5\%$};


            % Draw horizontal axis
            \draw[->] (-3.7,0) -- (3.7,0) node[right] {};
            % Draw vertical axis
            \draw[->] (0,0) -- (0,0.5) node[above] {};
            % Mark the center
            \node[below] at (0,0) {$0$};
            % Title
            \node[above] at (0,0.55) {\textbf{Standard Normal Distribution ($z$)}};
            \node at (0,0.55) {\textbf{with $\alpha = 0.05$}};
        \end{tikzpicture}
    \end{center}
\end{definitionbox}

\begin{examplebox}{Find crtitical value for the 95\% CI}{}
    For a confidence interval of 95\%, we want to find the z-score that leaves 2.5\% in each tail of the normal distribution.\\
    We want to find the $z$-value where the cumulative area )from the left up to that $z-score$) is $1 - 0.025 = 0.975$.\\
    We look in the $z$-tables for the value closest to $0.975$ and read the row and column headers to find the $z$-value.\\
    The $z$-value is $1.96$.\\
\end{examplebox}

\begin{examplebox}{Find the 95\% confidence interval for the population mean $\mu$ given}{}
    A dataset of 103 students, of whom 71 pay rent, was used to estimate the average weekly rent $\mu$.
    \begin{itemize}
        \item \textbf{Point estimate}: the sample mean $\bar{X} \approx 546.239$.
        \item \textbf{Sample standard deviation}: $s \approx 187.862$.
        \item \textbf{Sample size}: $n = 71$.
    \end{itemize}
    Confidence Interval is given by:
    $$\bar{X} \pm z_{\alpha/2} \times \frac{s}{\sqrt{n}}\Rightarrow 546.239 \pm 1.96 \times \frac{187.862}{\sqrt{71}}$$
    where $z_{\alpha/2} = 1.96$ for a 95\% confidence level.The resulting confidence interval is:
    $$(502.541, 589.938)$$
    \textbf{Interpretation}: We are 95\% confident that the true mean weekly rent for all NUI Galway students (population) is roughly 503 to 590 euros.
\end{examplebox}
\subsection{Higher Confidence Levels means Wider Intervals}
\begin{itemize}
    \item To achieve a \textbf{higher confidence level}, we need to increase the critical value $z_{\alpha/2}$, which in turn increases the margin of error.
    \item This results in a wider confidence interval, which means we are more certain that the true population parameter lies within that interval.
    \item Conversely a \textbf{lower confidence level} results in a smaller critical value, leading to a narrower confidence interval.
\end{itemize}
\pagebreak
\subsection{t-Distribution}
\begin{conceptbox}{Why the $t-$distribution}{}
    When the sample size is small ($n < 30$) and the population standard deviation $\sigma$ is unknown, simply substituting the sample standard deviation $s$ no longer suffices because the standard error is itself estimated with more uncertainty.\\[2ex]
    The $\textbf{t-distributution has thicker}$ tails than the normal distributution. This extra "fatness" in the tails accounts for the additional uncertainty in using $s$ instead of $\sigma$.
    \begin{center}
        \begin{tikzpicture}[
            domain=-3.5:3.5,
            xscale=0.5,
            yscale=3.5,
            smooth,
            line cap=round,
            line join=round
        ]
            % Draw axes
            \draw[->] (-3.7,0) -- (3.7,0) node[right] {$$};
            \draw[->] (0,0) -- (0,0.5) node[above] {$$};
            \node[below] at (0,0) {$0$};
        
            % Plot the Normal Distribution (dotted red)
            \draw[dotted, thick, custom_red] plot (\x, {0.4*exp(-(\x)^2/2)});
        
            % Plot the t-Distribution (solid blue) for df = 5.
            % The t-density: f(t) = (8/(3*pi*sqrt(5)))*(1+t^2/5)^(-3)
            \draw[thick, custom_blue] plot (\x, {(2/(3.14159*sqrt(3)))*((1+(\x)^2/3)^(-2))});
        
            % Title and legend
            \node[above] at (0,0.55) {\textbf{Normal vs.\ t-Distribution}};
        
            % Legend (placed at the top-right)
            \begin{scope}[shift={(3,0.4)}]
                \draw[dotted, thick, custom_red] (0,0) -- (1,0);
                \node[right] at (1,0) {Normal};
                \draw[thick, custom_blue] (0,-0.2) -- (1,-0.2);
                \node[right] at (1,-0.2) {t-Distribution ($df=3$)};
            \end{scope}
        \end{tikzpicture}        
    \end{center}
\end{conceptbox}
\vfill
\begin{conceptbox}{Degrees of Freedom (df)}{}
    A t-distribution is characterized by its degrees of freedom, where 
    $$df = n - 1 \quad \text{for a sample mean}$$
    As the sample size $n$ increases, the t-distribution approaches the standard normal distribution.\\
    For example, for $n = 30$, $df = 29$ and the t-distribution is very close to the normal distribution.\\
\end{conceptbox}
\vfill
\begin{definitionbox}{Confidece Intervals (t-based)}{}
    $$\bar{X} \pm t_{\alpha/2, df} \times \frac{s}{\sqrt{n}}$$
    where $t_{\alpha/2, df}$ is the critical value from the t-distribution with $df = n - 1$ degrees of freedom - or from a function like \texttt{qt()} in R.\\

    \textbf{Assumption}: The population itself should be approximately normally distributed when using t-based methods for small sample sizes.
\end{definitionbox}
\vfill
\begin{examplebox}{Finding t-critical values}{}
    Find the critical value for a 95\% confidence interval with $n = 12$ (so $df = 11$).\\
    We look for the row associated with $df = 11$ and the column associated with $\alpha/2 = 0.025$.\\
    The critical value is:
    $$t_{0.025, 11} \approx 2.201$$
\end{examplebox}
\pagebreak
\subsection{CI with large $\boldsymbol{n}$, and $\boldsymbol{\sigma}$ unknown}
The $z$-based critical interval is given as:
$$\bar{X} \pm z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}}$$
where $z_{\alpha/2}$ is the critical value from the standard normal distribution.
However, if the population standard deviation $\sigma$ is unknown, we can use the sample standard deviation $s$ as an estimate.\\
This gives us the following confidence interval:
$$\bar{X} \pm z_{\alpha/2} \times \frac{s}{\sqrt{n}}$$
\textbf{Interpretation}: around 95\% of all possible 95\% confidence intervals will contain the true population mean $\mu$. We can visualize that if we drew many repeated samples, sample means will form an overlapping $\mu$ and a small fraction will not.

\subsection{CI with small $\boldsymbol{n}$, and $\boldsymbol{\sigma}$ unknown}
If the sample size is small ($n < 30$) and the population standard deviation $\sigma$ is unknown, we use the t-distribution to construct the confidence interval. This gives us the following confidence interval:
$$\bar{X} \pm t_{\alpha/2, df} \times \frac{s}{\sqrt{n}}$$
where $t_{\alpha/2, df}$ is the critical value from the t-distribution with $df = n - 1$ degrees of freedom.\\
\textbf{Interpretation}: around 95\% of all possible 95\% confidence intervals will contain the true population mean $\mu$. We can visualize that if we drew many repeated samples, sample means will form an overlapping $\mu$ and a small fraction will not.

\begin{examplebox}{Turin Shroud}{}
    A historical cloth's age was tested by carbon dating on 12 pieces ($n = 12$). The sample mean was $x \approx 1261 \ AD$ and the sample standard deviation was $s \approx 61.2 \ AD$. Find the 95\% confidence interval for the population mean age of the cloth. \\[2ex]
    The standard error is given by:
    $$SE = \frac{s}{\sqrt{n}} = \frac{61.2}{\sqrt{12}} \approx 17.67$$
    For a 95\% confidence interval with $n-1 = 11$ degrees of freedom, the critical value is $t_{0.025, 11} \approx 2.201$.\\
    The confidence interval is given by:
    $$\bar{X} \pm t_{\alpha/2, df} \times SE = 1261 \pm 2.201 \times 17.67$$
    The resulting confidence interval is:
    $$(1222, 1300)$$
    \textbf{Interpretation}: The cloth’s true average carbon-dated age is plausibly within about 1222–1300 AD. This range casts doubt on claims that the cloth dates from centuries earlier.
\end{examplebox}
\begin{examplebox}{Unathorized Computer Acess}{}
    Find 95\% CI given:
    \begin{itemize}
        \item \textbf{Data}: 18 times between keystrokes 
        \item \textbf{Sample mean}: $\bar{X} = 0.29$ seconds
        \item Sample standard deviation: $s = 0.0074$ seconds
    \end{itemize}
    $$n  = 18 \Rightarrow df = 17$$
    For a 95\% confidence interval with $n-1 = 18$ degrees of freedom, the critical value is $t_{0.025, 17} \approx 1.740$.\\
    The resulting confidence interval is:
    $$(0.2532, 0.3268)$$
    \textbf{Interpretation}: We are 95\% confident that the true mean time between keystrokes is between 0.2532 and 0.3268 seconds.
\end{examplebox}


\subsection{When normality is questionable}
Recall that for small $n$, the t-distribution-based confidence interval requires data to be approximately normally distributed in the population. But many real datasets violate this assumption. - e.g. skewed data, heavily tailed data etc. \\[2ex]
\noindent Two broad remedies exist:
\begin{itemize}
    \item \textbf{Data transformation}: Apply a mathematical transformation to make the data more symmetric or bell shape (e.g.log-transformation). Then use t-based or z-based methods on the transformed scale.
    \item \textbf{Non-parametric methods}: Rely less on strict distributional assumptions. The bootstrap is a common and versatile non-parametric method approach to estimating confidence intervals and sampling variability.
\end{itemize}

\subsection{Data Transformations}
\textbf{Purpose}:
\begin{itemize}
    \item If the data has a strongly skewed or otherwise non-normal distribution, applying a suitable transformation (e.g. $\log(x)$, $\sqrt{x}$) can help to make the data more symmetric and bell-shaped.
    \item After the transformation, we can apply t-based or z-based methods can be applied more safely.
\end{itemize}
\textbf{Cautions}:
\begin{itemize}
    \item Finding the write transformation can be tricky; sometimes no simple transformation works well.
    \item Interpretation of results becomes more complex; if you compute a CI for the transformed mean, you must convert (e.g. exponentiate) the results back to the original scale.
    \item Despite these challenges, transformation often prove very useful in practice.
\end{itemize} 

\subsection{The Bootstrap}
\textbf{Motivation}:
\begin{itemize}
    \item Bootstrap methods do not require normality assumptions or a large $n$. They rely on the principle that the observed sample can server a reasonable proxy for the populations shape. 
    \item By resampling with replacement from the original sample (many times), one creates a "bootstrap distribution" that mimics the statistic (e.g. mean, median) of interest.
    \item This bootstrap distribution is then used to estimate hpw the statistic varies, allowing for confidence interval construction and hypothesis testing without explicit formulas.
\end{itemize}
\textbf{Basic Steps (Bootstrap Scheme)}
\begin{enumerate}
    \item \textbf{Resample with replacement}: Take a bootstrap sample of the same size $n$ as the original dataset, but drawn from the dataset with replacement.
    \item \textbf{Calculate Bootstrap statistic}: Compute the same summary measure of interest (e.g. mean, median) on the bootstrap sample.
    \item \textbf{Repeat}: Repeat steps (1) and (2) many times (e.g. 1000 times) to create a distribution of the bootstrap statistic.
    \item \textbf{Construct CI}: The bootstrap distribution of the resampled statistics can be used to determine the middle 95\% (or chosen confidence level) as the CI bounds.
\end{enumerate}


\textbf{Advantages}:
\begin{itemize}
    \item Works for all kinds of statistics (mean, median, proportion, regression coefficients, etc.) even when no closed-form CI exists.
    \item Far fewer assumptions about the underlying population distribution.
\end{itemize}
\textbf{Disadvantages}:
\begin{itemize}
    \item Computationally intensive; requires many resamples (e.g. 1000) to get a good approximation.
    \item Requires the sample itself to be a good representation of the population; if the sample is biased, the bootstrap may not work well.
\end{itemize}
\section{Inferential Statistics - Hypothesis Testing}
\subsection*{Recap: Confidence Intervals for a Population Mean}
\vfill
\begin{itemize}
    \item A \textbf{Confidence interval (CI)} provides a range of plausible values for a population parameter
    \item For a large sample ($n \geq 30$) or a known $\sigma$, we often use a z-based interval:
    $$\bar{X} \pm z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}}$$
    or replacing $\sigma$ with $s$ if $\sigma$ is unknown.
    \item For a small sample ($n < 30$) and unknown $\sigma$, we use a t-based interval:
    $$\bar{X} \pm t_{\alpha/2, df} \times \frac{s}{\sqrt{n}}$$
    where $df = n - 1$. Provided the population is approximately normal.
    \item If normality is questionable, we may use transformations or bootstrapping. 
\end{itemize}
\vfill
\subsection{Proportions}
\vfill
\begin{definitionbox}{Proportion}{}
    The \textbf{proportion} is a way to express the frequency of a specific outcome (labeled as “success”) relative to the total number of trials or observations. 
    $$p = \frac{X}{n}$$
    where $p$ is the proportion, $X$ is the number of successes, and $n$ is the total number of trials.
\end{definitionbox}
\vfill
\begin{conceptbox}{Why proportions?}{}
    Many outcomes are binary or categorical with two possible outcomes (e.g. success/failure, yes/no). Examples:
    \begin{itemize}
        \item Whether a student has a part-time job
        \item Whether a business has fallen victim to a scam
    \end{itemize}
    In such cases, we often estimate a population proportion $\pi$ of successes rather than a mean $\mu$.
\end{conceptbox} 
\vfill 
\subsubsection{Binomial Distribution}
\begin{conceptbox}{Bernoulli Trials}{}
   When we repeat an experiment or observation, each trial is assumed to be independent and has two possible outcomes. If each trial has a probability of $\pi$ success, these trials are called \textbf{Bernoulli trials}.
\end{conceptbox}
\vfill
\noindent If we perform $n$ independent Bernoulli trials, the number of successes $X$ follows a \textbf{binomial distribution} with $n$, the number of trials and $\pi$,the probability of success on each trial.
$$X \sim B(n, \pi)$$
This tells us how likely we are to observe a certain number of successes in $n$ trials.\\[2ex]
\textbf{Link to Proportion}: \\
The sample proportion $p$ is just the normalized version of $X$, calculated by $p = \frac{X}{n}$. It provides a direct, interpretable measure of success rate in the sample.
\pagebreak
\subsubsection{Normal Approximation of the Sample Proportion}
\textbf{When is the normal approximation valid?}\\
The approximation of the distribution of $p$ by a normal distribution is valid when both of the following conditions are met:   
$$n\pi \geq 5 \quad \text{and} \quad n(1 - \pi) \geq 5$$
These conditions ensure there are enough successes and failures for the approximation to hold. \\[2ex]
\textbf{How does it work?}
Since $X$ is binomially distributed, its mean is $n\pi$ and its variance is $n\pi(1 - \pi)$. When we convert $X$ into the proportion $p$, the mean and variance transform as follows:
\begin{itemize}
    \item Mean of $p$: $E(p) = \frac{E(X)}{n} = \pi$
    \item Variance of $p$: $Var(p) = \frac{Var(X)}{n^2} = \frac{\pi(1 - \pi)}{n}$
\end{itemize}
For large $n$ (above conditions), the distribution of $p$ can be approximated by a normal distribution:
$$p \sim N\left(\pi, \frac{\pi(1 - \pi)}{n}\right)$$\\
\textbf{Interpretation}: \\
This approximation means if we were to make many samples of size $n$, the distribution of the same proportions would cluster around the true proportion, $pi$, with variability decreasing as the sample size $n$ increases. This normality is what allows statisticians to construct confidence intervals and perform hypothesis tests on population proportions.
\subsubsection{Confidence Intervals for Proportion $\pi$}
For a large sample size where $np$ and $n(1-p)$ are both greater or equal to 5, a 95\% C.I for the population proportion $\pi$ is given by:
$$p \pm z_{\alpha/2} \times \sqrt{\frac{p(1 - p)}{n}}$$
where:
\begin{itemize}
    \item $p$ is the sample proportion (e.g. $\frac{X}{n}$)
    \item $z_{\alpha/2}$ is the critical value from the standard normal distribution (e.g. $1.96$ for 95\% confidence)
    \item The quantity under the square root is the standard error of the sample proportion.
\end{itemize}

\begin{examplebox}{Financial Scams}{}
    A survey of $n=80$ small businesses found that $X = 16$ had fallen victim to a financial scam. Find the 95\% confidence that all small businesses have fallen victim to this scam. \\[2ex]

    \begin{itemize}
        \item Sample proportion: $p = \frac{X}{n} = \frac{16}{80} = 0.20$
        \item Standard Error = $SE = \sqrt{\frac{p(1 - p)}{n}} = \sqrt{\frac{0.20(1 - 0.20)}{80}} = \sqrt{\frac{0.20 \times 0.80}{80}} \approx 0.05$
        \item For a 95\% confidence interval $\alpha = 0.05, z_{\alpha/2} = 1.96$.
    \end{itemize}
    The 95\% confidence interval is given by:
    $$p \pm z_{\alpha/2} \times SE = 0.20 \pm 1.96 \times 0.05$$
    The resulting confidence interval is:
    $$\approx (0.10, 0.30)$$
    \textbf{Interpretation}: We are 95\% confident that between 10\% and 30\% of all small businesses have fallen victim to this scam.
\end{examplebox}
\begin{conceptbox}{Proportion CI Test IN R}{}
    The function \texttt{prop.test(x, n, conf.level, correct=False)}  gives a confidence interval for a proportion.
\end{conceptbox}
\pagebreak
\subsubsection{Maximizing the Standard Error}
The standard error for a proportion $p$ is given by:
$$SE = \sqrt{\frac{p(1 - p)}{n}}$$
This maximizes at $p = 0.5$. Thus the worst-case margin of error for a 95\% confidence interval is:
$$\approx \pm 2 \times \sqrt{\frac{0.5\times0.5}{n}} = \pm \frac{1}{\sqrt{n}}$$
\textbf{Rule of thumb}: for $n = 1000$, the margin of error is about $1/\sqrt{1000} \approx 0.03$, i.e. 3\% error.
\subsection{Confidence Intervals for Counts}
\subsubsection{Possion Setup}
A count variable $X$, over a fixed interval (e.g. "number of emails per day") often follows a \textbf{Poisson} distribution. with parameter $\lambda$.  \\
Recall $X \sim Poisson(\lambda)$ implies $E(X) = \lambda$ and $Var(X) = \lambda$.
\subsubsection{Central Limit Theorem Approximation}
For large enough $\lambda$, the Central Limit Theorem, implies the sample mean of a Possion variable is approximately normally distributed:
$$X \sim N(\lambda, \frac{\lambda}{n})$$
If we have $n$ observations of some Poisson process, the overall mean $\bar{\lambda}$ is used to estimate the population mean $\lambda$. \\[2ex]
\textbf{Criteria}: The product $n\lambda$ should be sufficiently large (e.g. $\geq 50$) for the approximation to hold well.
\begin{examplebox}{Emails per Day}{}
    Given a sample of $n=64$ students with a mean of $\bar{\lambda} = 53$ emails per day, find the 95\% confidence interval for the population mean $\lambda$.\\[2ex]

    Standard Error: 
    $$SE = \sqrt{\bar{\lambda}} = \sqrt{53} \approx 7.28$$
    For a 95\% confidence interval $\alpha = 0.05, z_{\alpha/2} = 1.96$.
    The 95\% confidence interval is given by:
    $$\bar{\lambda} \pm z_{\alpha/2} \times SE = 53 \pm 1.96 \times 7.28$$
    The resulting confidence interval is:
    $$(38.7, 67.3)$$
    \textbf{Interpretation}: We are 95\% confident that the true mean number of emails per day for all students is between 39 and 67.
    
\end{examplebox}

\pagebreak
\subsection{Hypothesis Testing}
Hypothesis testing is a systematic framework to evaluate claims about a population parameter (e.g. a population mean $\mu$).
\begin{examplebox}{Introdcutory Example}{}
    A claim has been made that students, on average, have been in 4 different relationships. \\
    The corresponding 95\% CI is $[2.7, 3.7]$ which did not include 4, suggesting the claim was unlikely to be true.
\end{examplebox}
Hypothesis testing relies on evidence (sample data) to decide whether a claim (the "null hypothesis") is plausible. Confidence intervals and hypothesis tests are the main building blocks of statistical inference.
\subsubsection{Hypothesus Testing Basics}
\textbf{Purpose}: \\
Asses whether a parameter (often the mean, $\mu$) is equal to a specific value or if it differs in a particular direction  \\[2ex]
\textbf{Null and Alternative Hypotheses}:
\begin{itemize}
    \item \textbf{Null hypothesis} ($H_0$): Usually states "no difference" or "no effect" or a specific claimed value (e.g. $\mu = 6.5$) 
    \item \textbf{Alternative Hypothesis} ($H_1$ or $H_a$) Represents what we suspect or want to test for, such as $\mu \neq 6.5$, $\mu < 6.5$, or $\mu > 6.5$.
\end{itemize}

\end{document}
