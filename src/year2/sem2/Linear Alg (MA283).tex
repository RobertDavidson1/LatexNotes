\documentclass[a4paper, 9pt]{extarticle}
\input{../../preamble.sty}
\input{../../macros.sty}

\title{
\textbf{MA283: Linear Algebra} \\ 
}

\usepackage{geometry}
 \geometry{
 a4paper,
 bottom=22mm,
 }

\author{
  70\% Exam\\
30\% Continuous Assessment (Homework) \\
10\% Optional Project (Bonus)\\ [2ex]
Robert Davidson
}     
\date{}       % Optional: Add date if desired

\begin{document}

\maketitle
\pagebreak

\tableofcontents
\pagebreak
\section{Review of Matrix Algebra}
\subsubsection*{Fields}
\begin{itemize}
  \item A field $F$ is a set where addition, subtraction, multiplication and division (by nonzero elements)  satisfy the usual algebraic properties. Common fields include $\mathbb{R}$ and $\mathbb{C}$
  \item We write $\mathbb{F}^p$ for the vector space of all $p$ vectors with entries in $\mathbb{F}$.
  \item We'll cheat and treat any ordered list of $p$ elements of $\mathbb{F}$ as an element of $\mathbb{F}^p$.
  \item For example, in $\mathbb{R}^3$, we might consider $(1,2,3)$ as coordinates, a row vector, or a column vector with 3 real entries.
\end{itemize}


\subsubsection*{Matrices Over a Field}
\begin{itemize}
  \item An $m \times n$ matrix over a field $\mathbb{F}$ is an array of $m$ rows and $n$ columns of elements from $\mathbb{F}$.
  \item When $m = n$, we write $M_n(\mathbb{F})$, otherwise we write $M_{m \times n}(\mathbb{F})$.
\end{itemize}
\subsubsection*{Addition and Scalar Multiplication}
\begin{itemize}
  \item Two matrices of the same size $m \times n$ can be added entrywise
  \item The $m \times n$ matrix has all entries equal to zero and acts as the additive identity (adding it to any matrix does not change the matrix)
  \item Multiplying a matrix by a scalar means multiplying each entry by that scalar
  \item The set of all $m \times n$ matrices over $\mathbb{F}$ is a vector space over $\mathbb{F}$
\end{itemize}
\subsubsection*{Linear Combinations}
\begin{itemize}
  \item A linear combination of vectors $v_1, v_2, \ldots, v_k$ in a vector space $V$ with coefficients $a_1, a_2, \ldots, a_k \in \mathbb{F}$ is defined as:
        $$ a_1v_1 + a_2v_2 + \ldots + a_kv_k$$
  \item In particular, matrices themselves can be combined linearly, (e.g. $2A - 3B$)
\end{itemize}
\subsubsection*{Row and Column Vectors}
\begin{itemize}
  \item A column vector is a matrix with one column
  \item A row vector is a matrix with one row
\end{itemize}
\subsubsection*{Matrix-Vector Multiplication}
\begin{itemize}
  \item If $A$ is $m \times n$ matrix and $v$ is an $n$-entry column vector, the product $Av$ is defined by taking a linear combination of the columns of $A$ with the entries of $v$ as coefficients.
  \item The result $Av$ is an $m$-entry column vector.
  \item For a row vector $u$ with $m$ entries, and an $m \times n$ matrix $A$m the product $uA$ a row vector in $\mathbb{F}^n$ formed by the linear combination of the rows of $A$ with the entries of $u$ as coefficients.
\end{itemize}
\subsubsection*{Matrix-Matrix Multiplication}
\begin{itemize}
  \item If $A$ is a $m \times p$ and $B$ is a $p \times n$ matrix, the product $AB$ is defined only when the inner dimensions match $(p)$
  \item To find each column of $AB$, multiply $A$ with the corresponding column vector of $B$.
  \item In entrywise form:
        $$
          (AB)_{ij} = A_{i,1}B_{1,j} + A_{i,2}B_{2,j} + \ldots + A_{i,p}B_{p,j} = \sum_{k=1}^p A_{i,k}B_{k,j}
        $$
\end{itemize}

\subsubsection*{Dot Product and Orthogonality}
\begin{itemize}
  \item For two $p$-entry vectors, $u, v \in \mathbb{F}^p$, their dot product is:
        $$u \cdot v = \sum_{k=1}^p u_k v_k$$
  \item Vectors are \textbf{orthogonal} if their dot product is zero.
  \item If $\mathbb{F} = \mathbb{R}$, this means the vector are perpendicular.
  \item In matrix multiplication, the entry $(AB)_{ij}$ can be viewed as the dot product of Row $i$ with Column $j$ of $B$.
\end{itemize}
\subsubsection*{Matrices and Tables}
Lets consider the table that gives the numbers of Maths $M$, Physics $P$ and Chemistry $C$ students in each of the 3 years of a course: \\[2ex]
\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{tabularx}{0.6\textwidth}{c | c c c}
      \toprule
      Year & M  & P   & C  \\
      \midrule
      2015 & 50 & 100 & 70 \\
      2016 & 60 & 80  & 80 \\
      2017 & 70 & 90  & 90 \\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \begin{center}
    $$\mathrm{A}=
      \begin{bmatrix}
        50 & 100 & 70 \\
        60 & 80  & 80 \\
        80 & 70  & 70
      \end{bmatrix}
    $$
  \end{center}
\end{minipage} \\[2ex]
Each student of $M, P, C$ must also take a course in $X$ and $Y$. We can represent the credits they earn as a matrix: \\[2ex]
\begin{minipage}
  {0.5\textwidth}
  \begin{center}
    \begin{tabularx}{0.6\textwidth}{c | c c}
      \toprule
      Subject & X  & Y  \\
      \midrule
      M       & 10 & 0  \\
      P       & 15 & 15 \\
      C       & 20 & 10 \\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \begin{center}
    $$\mathrm{B}=
      \begin{bmatrix}
        10 & 0  \\
        15 & 15 \\
        20 & 10
      \end{bmatrix}
    $$
  \end{center}
\end{minipage} \\[2ex]
The total number of credits earned each year can be found by the matrix product $AB$:
$$
  AB =
  \begin{bmatrix}
    50 & 100 & 70 \\
    60 & 80  & 80 \\
    80 & 70  & 70
  \end{bmatrix}
  \begin{bmatrix}
    10 & 0  \\
    15 & 15 \\
    20 & 10
  \end{bmatrix}
  =
  \begin{bmatrix}
    50 \cdot 10 + 100 \cdot 15 + 70 \cdot 20 & 50 \cdot 0 + 100 \cdot 15 + 70 \cdot 10 \\
    60 \cdot 10 + 80 \cdot 15 + 80 \cdot 20  & 60 \cdot 0 + 80 \cdot 15 + 80 \cdot 10  \\
    80 \cdot 10 + 70 \cdot 15 + 70 \cdot 20  & 80 \cdot 0 + 70 \cdot 15 + 70 \cdot 10
  \end{bmatrix}
$$
We can represent the result as a table: \\[2ex]
\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{tabularx}{0.6\textwidth}{c | c c}
      \toprule
      Year & X credits & Y credits \\
      \midrule
      2015 & 3400      & 2200      \\
      2016 & 3400      & 2000      \\
      2017 & 3250      & 1750      \\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \begin{center}
    $$\mathrm{A}=
      \begin{bmatrix}
        3400 & 2200 \\
        3400 & 2000 \\
        3250 & 1750 \\
      \end{bmatrix}
    $$
  \end{center}
\end{minipage}

\subsubsection*{Linear Transformations}
Let $m$ and $n$ be positive integers, A linear transformation $T$ from $\mathbb{R}^n$ to $\mathbb{R}^m$, denoted $T: \mathbb{R}^n \to \mathbb{R}^m$, is a function that satisfies the following properties:
\begin{itemize}
  \item $T(u+v) = T(u) + T(v)$
  \item $T(\lambda u) = \lambda T(u)$
\end{itemize}
$\forall u, v \in \mathbb{R}^n$ and scalars $\lambda \in \mathbb{R}$ \\[2ex]
When $T : \mathbb{R}^3 \to \mathbb{R}^2$, if we know $T$ applied to the three standard basis vectors of $\mathbb{R}^3$:
$$e_1 = \begin{bmatrix}
    1 \\
    0 \\
    0
  \end{bmatrix}, \quad
  e_2 = \begin{bmatrix}
    0 \\
    1 \\
    0
  \end{bmatrix}, \quad
  e_3 = \begin{bmatrix}
    0 \\
    0 \\
    1
  \end{bmatrix}$$
we can form a $2 \times 3$ matrix $A$ whose columns are exactly these image,then $T(v) = Av$ for any column vector $v \in \mathbb{R}^3$.
\subsubsection*{Composition of Linear Transformations}
\begin{itemize}
  \item If $T: \mathbb{R}^n \to \mathbb{R}^p$ and $S: \mathbb{R}^p \to \mathbb{R}^m$, then the composition $(S \circ T)(v) = S(T(v))$
  \item If $T$ is represented by a $p \times n$ matrix $A$ and $S$ by an $m \times p$ matrix $B$ then the composition $S \circ T$ is represented by the matrix product $BA$.
  \item Also $(AB)C = A(BC)$
  \item Composing transformations is only possible if the codomain of the first transformation matches the domain of the second transformation, that is:
        $$A \in M_{m \times n} \quad B \in M_{p \times m} \quad \Rightarrow \quad AB \in M_{m \times n}$$
\end{itemize}
\subsubsection*{The $n \times n$ Identity Matrix}
$I_n$ has 1s on the main diagonal:
$$I_2 =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1
  \end{bmatrix}, \quad
  I_4 =
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix}
$$
The identity matrix is the \textbf{neutral element} for multiplication:
$$
  A \cdot I_n = A \quad \text{and} \quad I_n \cdot B = B \quad \text{(where $A$ has $n$ columns and $B$ has $n$ rows)}
$$
The is interpreted as the \textbf{identity transformation} on $\mathbb{R}^n$, so composing with it has no effect on any linear map.
\subsubsection*{Invertible (Non-Singular) Matrices}
A square $n \times n$ matrix $A$ has an inverse $A^{-1}$ if there exists another $n \times n$ matrix such that:
$$AB = I_n \quad \text{and} \quad BA = I_n$$
If $A$ has an inverse, we say it is \textbf{unique}; there cannot be two different inverses for the same matrix. \\[2ex]
Not all matrices are invertible. A key fact (explained later) is that:
$$A \text{ is invertible} \quad \Leftrightarrow \quad \text{the determinant $\neq 0$}$$
\subsubsection*{Transpose of a Matrix}
FGor a $m \times n$ matrix $A$, the transpose $A^T$ is the $n \times m$ matrix obtained by turning the rows of $A$ into the columns of $A^T$:
$$
  \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
  \end{bmatrix}^T =
  \begin{bmatrix}
    1 & 4 \\
    2 & 5 \\
    3 & 6 \\
  \end{bmatrix}
$$


\section{Systems of linear equations}
\subsection{Linear equations and Solution Sets}
A linear equation in the variables $x$ and
$y$ is an equation of the form
\begin{equation*}
  2x + y = 3
\end{equation*}
If we replace $x$ and $y$ with some numbers, the statement \textbf{becomes true or false}.

\begin{definitionbox}{Solution to a linear equation}{}
  A pair, $(x_0, y_0) \in \mathbb{R}$, is a solution to an linear equation if setting $x = x_0$ and $y = y_0$ \textbf{makes the equation true.}
\end{definitionbox}

\begin{definitionbox}{Solution set}{}
  The \textbf{solution set} is the set of all solutions to a linear equation.
  $$a_1X_1 + a_2X_2 + \ldots + a_nX_n = b \quad \text{where} \; a_i, b \in \mathbb{R}$$
  is an \textbf{affine hyperplane} in $\mathbb{R}^n$; geometrically resembles a copy of $\mathbb{R}^{n-1}$ inside $\mathbb{R}^n$.
\end{definitionbox}
\subsection{Elementary Row Operations}
To solve a system of linear equations we associate an \textbf{augmented matrix} to the system of equations. For example:
$$
  \begin{array}
    {ccccccc}x & + & 2y & - & z  & = & 5 \\
    3x         & + & y  & - & 2z & = & 9 \\
    -x         & + & 4y & + & 2z & = & 0
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1  & 2 & -1 & | & 5 \\
    3  & 1 & -2 & | & 9 \\
    -1 & 4 & 2  & | & 0
  \end{bmatrix}
$$
To solve, we can perform the following \textbf{Elementary Row Operations (EROs)}:
\begin{enumerate}
  \item Multiply a row by a non-zero constant.
  \item Add a multiple of one row to another row.
  \item Swap two rows.
\end{enumerate}
The goal of these operations is to transform the augmented matrix into \textbf{row echelon form} (REF) or \textbf{reduced row echelon form} (RREF).

\subsubsection{REF and Strategy}
\begin{minipage}{0.8\textwidth}
  We say a matrix is in \textbf{row echelon form} (REF) if:
  \begin{itemize}
    \item The first non zero entry in each row is a 1 (called the \textbf{leading 1}).
    \item If a column has a leading 1, then all entries below it are 0.
    \item The leading 1 in each row is to the right of the leading 1 in the previous row.
    \item All rows of 0s are at the bottom of the matrix.
  \end{itemize}
\end{minipage}
\begin{minipage}{0.2\textwidth}
  \begin{center}
    $$
      \begin{bmatrix}
        1 & 2 & -1 & | & 3  \\
        0 & 1 & 2  & | & -1 \\
        0 & 0 & 1  & | & -1
      \end{bmatrix}
    $$
    \emph{Example of REF}
  \end{center}


\end{minipage} \\[2ex]
We have produced a new system of equations. This is easily solved by back substitution.

\begin{conceptbox}{Stategy for Obtaining REF}{}
  \begin{itemize}
    \item Get a 1 as the top left entry
    \item Use this 1 to clear the entries below it
    \item Move to the next column and repeat
    \item Continue until all leading 1s are in place
    \item Use back substitution to solve the system
  \end{itemize}
\end{conceptbox}
\subsubsection{Row Reduced Echelon Form}
\begin{minipage}{0.8\textwidth}
  A matrix is in \textbf{reduced row echelon form} (RREF) if:
  \begin{itemize}
    \item It is in REF
    \item The leading 1 in each row is the only non-zero entry in its column.
  \end{itemize}
\end{minipage}
\begin{minipage}{0.2\textwidth}
  \begin{center}
    $$
      \begin{bmatrix}
        1 & 0 & 0 & | & 2  \\
        0 & 1 & 0 & | & -1 \\
        0 & 0 & 1 & | & -1
      \end{bmatrix}
    $$
    \emph{Example of RREF}
  \end{center}
\end{minipage}

\subsection{Leading variables and free variables}
We'll start by an example:
$$
  \begin{array}{ccccccccc}
    x_1  & - & x_2  & - & x_3  & + & 2x_4 & = & 0 \\
    2x_1 & + & x_2  & - & x_3  & + & 2x_4 & = & 8 \\
    x_1  & - & 3x_2 & + & 2x_3 & + & 7x_4 & = & 2
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1 & -1 & -1 & 2 & | & 0 \\
    2 & 1  & -1 & 2 & | & 8 \\
    1 & -3 & 2  & 7 & | & 2
  \end{bmatrix}
$$
Solving this system of equations, we get:
$$
  \text{RREF:}\quad
  \begin{bmatrix}
    1 & 0 & 0 & 2  & | & 4 \\
    0 & 1 & 0 & -1 & | & 2 \\
    0 & 0 & 0 & 1  & | & 2
  \end{bmatrix}
  \quad \Rightarrow \quad
  \begin{array}{ccccc}
    x_1 & + & 2x_4 & = & 4 \quad \\
    x_2 & - & x_4  & = & 2 \quad \\
    x_3 & + & x_4  & = & 2 \quad
  \end{array}
  \quad \Rightarrow \quad
  \begin{array}{ccc}
    x_1 & = & 4 - 2x_4 \\
    x_2 & = & 2 + x_4  \\
    x_3 & = & 2 - x_4
  \end{array}
$$
This RREF tells us how the \textbf{leading variables} ($x_1, x_2, x_3$) depend on the \textbf{free variable} ($x_4$). The free variable can take any value in $\mathbb{R}$. We write the solution set as:
$$x_1 = 4 - 2t, \quad x_2 = 2 + t, \quad x_3 = 2 - t, \quad x_4 = t \quad \text{where} \; t \in \mathbb{R}$$
$$(x_1, x_2, x_3, x_4) = (4-2t, 2+t, 2-t, t); \quad t \in \mathbb{R}$$
\begin{definitionbox}{Leading and Free Variables}{}
  \begin{itemize}
    \item  \textbf{Leading variable} : A variable whose columns in the RREF contain a leading 1
    \item \textbf{Free variable} : A variable whose columns in the RREF do not contain a leading 1
  \end{itemize}
\end{definitionbox}

\subsection{Consistent and Inconsistent Systems}
Consider the following system of equations:
$$
  \begin{array}{ccccccc}
    3x & + & 2y & - & 5z & = & 4 \\
    x  & + & y  & - & 2z & = & 1 \\
    5x & + & 3y & - & 8z & = & 6
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    3 & 2 & -5 & | & 4 \\
    1 & 1 & -2 & | & 1 \\
    5 & 3 & -8 & | & 6
  \end{bmatrix}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1 & 1 & -2 & | & 1 \\
    0 & 1 & -1 & | & 1 \\
    0 & 0 & 0  & | & 1
  \end{bmatrix} \quad \text{(REF)}
$$
We can see the last row of the REF is:
$$
  0x + 0y + 0z = 1
$$
This equation clearly has no solution, and hence the system has no solutions. We say the system is \textbf{inconsistent}. Alternatively, we say the system is \textbf{consistent} if it has at least one solution.
\subsection{Possible Outcomes when solving a system of equations}
\begin{itemize}
  \item The system may be \textbf{inconsistent} (no solutions) - i.e:
        $$ [0 \; 0 \; \dots \; 0 \; | \; a] \quad a \neq 0$$
  \item The system may be \textbf{consistent} which occurs if:
        \begin{itemize}
          \item \textbf{Unique Solutions} each column (aside from the rightmost) contains a single leading 1. - i.e:
                $$
                  \begin{bmatrix}
                    1 & 0 & 0 & | & 4  \\
                    0 & 1 & 0 & | & 3  \\
                    0 & 0 & 1 & | & -2
                  \end{bmatrix}
                $$

          \item \textbf{Infinitely many solutions} at least one variable does not appear as a leading 1 in any row, making it a free variable - i.e:
                $$
                  \begin{bmatrix}
                    1 & 2 & -1 & | & 3  \\
                    0 & 0 & 1  & | & -2 \\
                    0 & 0 & 0  & | & 0
                  \end{bmatrix}
                $$
        \end{itemize}
\end{itemize}

\subsection{Elementary Row Operations as Matrix Transformations}
Elementary row operations may be interpreted as \textbf{matrix multiplication}. To see this, first we introduce the \textbf{Identity matrix:}
\vspace{-2ex}
$$I_3 = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix}$$
The $I_m$ Identity matrix is an $m \times m$ matrix with 1s on the diagonal and 0s elsewhere. We also introduce the $E_{i,j}$ matrix which has $1$ in the $(i,j)$ position and $0$s elsewhere. For example:
$$
  E_{1,2} = \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
$$
Then:
\vspace{-2ex}
$$I_3 + 4E_{1,2} = \begin{bmatrix}
    1 & 4 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix}$$
Performing a row operation on $A$ is the same as multiplying $A$ by an appropriate matrix $E$ on the left.
These matrices are called \textbf{elementary matrices}. They are \textbf{always invertible,} and \textbf{their inverses are also elementary matrices}. The statement:
$$
  "\emph{every matrix can be reduced to RREF through EROs}"$$
is equivalent to saying that
\begin{align*}
  "\emph{for every matrix $A$ with $m$ rows, there exists a $m \times m$ matrix $B$} \\
  \emph{which is a product of elementary matrices such that $BA$ is in RREF.}"
\end{align*}
\vspace{-8ex}
\subsubsection{Multiplying a Row by a Non-Zero Scalar}
When multiplying row $i$ of matrix $A$ by a scalar $\alpha \neq 0$, we can use the matrix:
$$I_m + (\alpha - 1)E_{i,i}$$
This works because it modifies only the $(i,i)$ entry of the identity matrix to be $\alpha$ while keeping all other entries unchanged. When multiplied with $A$, it scales row $i$ by $\alpha$ and leaves all other rows intact.\\
\textbf{Example:} If $\alpha = 5$ and $i = 2$, then:
\vspace{-2ex}
$$
  I_3 + 4E_{2,2} = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 5 & 0 \\
    0 & 0 & 1
  \end{bmatrix}\
  \quad
  A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad \quad\quad
  (I_3 + 4E_{2,2})A = \begin{bmatrix}
    1  & 2  & 3  \\
    20 & 25 & 30 \\
    7  & 8  & 9
  \end{bmatrix}$$
\vspace{-4ex}
\subsubsection{Switching Two Rows}
To swap rows $i$ and $k$, we use:
$$S = I_m + E_{i,k} + E_{k,i} - E_{i,i} - E_{k,k}$$
This works by:
\begin{itemize}
  \item Removing the 1's at positions $(i,i)$ and $(k,k)$ from the identity matrix
  \item Adding 1's at positions $(i,k)$ and $(k,i)$
\end{itemize}
\vspace{-1ex}
\textbf{Example:} Swapping rows 1 and 3:
\vspace{-2ex}
$$  S = \begin{bmatrix}
    0 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 0
  \end{bmatrix}
  \quad
  A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad \quad\quad
  SA = \begin{bmatrix}
    7 & 8 & 9 \\
    4 & 5 & 6 \\
    1 & 2 & 3
  \end{bmatrix}$$
\vspace{-5ex}
\subsubsection{Adding a Multiple of One Row to Another}
To replace row $k$ with row $k + \alpha \times$ row $i$, use:
$$I_m + \alpha E_{k,i}$$
This adds $\alpha$ times row $i$ to row $k$ while leaving all other rows unchanged because:
\begin{itemize}
  \item For any row $j \neq k$, the corresponding row in this matrix is just the standard basis row
  \item Row $k$ becomes the sum of the standard basis row $k$ plus $\alpha$ times the standard basis row $i$
\end{itemize}
\textbf{Example:} Adding 3 times row 1 to row 2:
\vspace{-1ex}
$$ I_3 + 3E_{2,1} = \begin{bmatrix}
    1 & 0 & 0 \\
    3 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix} \quad A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad\quad\quad
  (I_3 + 3E_{2,1})A = \begin{bmatrix}
    1 & 2  & 3  \\
    7 & 11 & 15 \\
    7 & 8  & 9
  \end{bmatrix}$$

\begin{examplebox}{}{}
  Write the inverse of an elementary matrix and show it is an elementary matrix.
  \\ \rule{\textwidth}{1px} \\
  \textbf{Multiplying a row by a nonzero scalar:}
  \begin{itemize}
    \item \textbf{Operation:} Multiply row $i$ by $\alpha \neq 0$.
    \item \textbf{Elementary Matrix:} $E = I_m + (\alpha - 1)E_{i,i}$
    \item \textbf{Inverse:} To reverse the operation, multiply row $i$ by $1\backslash\alpha$. Hence, the inverse is
          $$
            E^{-1} = I_m + \left(\frac{1}{\alpha} - 1\right)E_{i,i}  =\; I_m + \frac{1-\alpha}{\alpha}\,E_{i,i}.
          $$
  \end{itemize}
  \textbf{Swapping two rows:}
  \begin{itemize}
    \item \textbf{Operation:} Swap rows $i$ and $k$.
    \item \textbf{Elementary Matrix:} $S = I_m - E_{i,i} - E_{k,k} + E_{i,k} + E_{k,i}$
    \item \textbf{Inverse:} Since swapping the same two rows twice returns them to their original positions,
          $$
            S^{-1} = S.
          $$
  \end{itemize}
  \textbf{Adding a multiple of one row to another:}
  \begin{itemize}
    \item \textbf{Operation:} Add $\alpha$ times row $i$ to row $k$.
    \item \textbf{Elementary Matrix:} $E = I_m + \alpha\,E_{k,i}$
    \item \textbf{Inverse:} To undo the operation, subtract $\alpha$ times row $i$ from row $k$. Therefore,
          $$
            E^{-1} = I_m - \alpha\,E_{k,i}.
          $$
  \end{itemize}
\end{examplebox}
\begin{examplebox}{}{}
  Prove that every invertible matrix in $M_n(\mathbb{R})$ is a product of elementary matrices.
  \\ \rule{\textwidth}{1px} \\
  Let $A$  be an invertible matrix in $M_n(\mathbb{R})$. Since $A$ is invertible, we can use Gaussian elimination to transform $A$ into the identity matrix $I_n$. \\[2ex]
  Let $E_1, E_2, \ldots, E_k$ be the elementary matrices corresponding to the row operations used in the elimination process. Then, we have:
  \begin{align*}
    \text{Multiplying a row by a scalar:}           & \quad I_n + (\alpha - 1)E_{i,i}                   \\
    \text{Swapping two rows:}                       & \quad I_n + E_{i,k} + E_{k,i} - E_{i,i} - E_{k,k} \\
    \text{Adding a multiple of one row to another:} & \quad I_n + \alpha\,E_{k,i}
  \end{align*}
  Applying these in sequence to $A$ gives:
  $$E_k \cdots E_2 E_1 A = I_n$$
  Since $E_k \cdots E_2 E_1 = I_n$, we can multiply both sides by $(E_k \cdots E_2 E_1)^{-1}$ on the left to obtain:
  $$A = (E_k \cdots E_2 E_1)^{-1} I_n = (E_k \cdots E_2 E_1)^{-1}$$
  Using the property:
  $$(E_k \cdots E_2 E_1)^{-1} = E_1^{-1} E_2^{-1} \cdots E_k^{-1},$$
  we can express $A$ as a product of elementary matrices:
  $$A = E_1^{-1} E_2^{-1} \cdots E_k^{-1}$$
  Since each $E_i$ is an elementary matrix, its inverse is also an elementary matrix. Therefore, $A$ can be expressed as a product of elementary matrices.
\end{examplebox}
\pagebreak
\subsection{EROs and Inverses}
Elementary Row Operations can be used to find the inverse of a square matrix.  Consider a square matrix $A \in M_n(\mathbb{F})$ (that is, an $n \times n$ matrix over a field $\mathbb{F}$). If $A$ is invertible, let
$$
  A^{-1}
  =
  \begin{bmatrix}
    |            & |            &        & |            \\
    \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \\
    |            & |            &        & |
  \end{bmatrix}
$$
be its inverse, where each $\mathbf{v}_i$ is the $i$th column of $A^{-1}$. By definition of the matrix inverse, we have
$$
  A \, A^{-1}
  =
  A \begin{bmatrix}
    \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n
  \end{bmatrix}
  =
  \begin{bmatrix}
    A \mathbf{v}_1 & A \mathbf{v}_2 & \cdots & A \mathbf{v}_n
  \end{bmatrix}
  =
  I_n,
$$
the $n \times n$ identity matrix. This implies that
$$
  A \mathbf{v}_i = \mathbf{e}_i, \quad \text{for each } i=1, 2, \ldots, n,
$$
where $\mathbf{e}_i$ is the $i$th column of $I_n$ (which has a 1 in the $i$th row and 0 everywhere else). In other words, each column $\mathbf{v}_i$ of $A^{-1}$ is the unique solution to the linear system
$$
  A \mathbf{v}_i = \mathbf{e}_i.
$$
To find $A^{-1}$ effectively, we form the augmented matrix $[A \mid I_n]$ and apply EROs to trasnform $A$ into $I_n$. When this is achieved, the augmented portion becomes $A^{-1}$. Thus, we have
$$
  \text{RREF}\bigl([\,A \mid I_n\,]\bigr)
  =
  \bigl[\, I_n \mid A^{-1} \bigr].
$$
\begin{examplebox}{}{}
  Find $A^{-1}$ if $A = \begin{bmatrix}
      3 & 4 & -1 \\
      1 & 0 & 3  \\
      2 & 5 & -4
    \end{bmatrix}$.
  \\[2ex] \rule{\textwidth}{1px} \\
  We form a $3 \times 6$ matrix $A' = [A \mid I_3]$:
  $$
    A' = \begin{bmatrix}
      3 & 4 & -1 & | & 1 & 0 & 0 \\
      1 & 0 & 3  & | & 0 & 1 & 0 \\
      2 & 5 & -4 & | & 0 & 0 & 1
    \end{bmatrix}
  $$
  We apply the following EROs to $A'$:
  \begin{itemize}
    \item $R_1 \leftrightarrow R_2$
    \item $R_2 \rightarrow R_2 - 3R_1$
    \item $R_3 \rightarrow R_3 - 2R_1$
    \item $R_3 \rightarrow R_3 + R-2$
    \item $R_3 \leftrightarrow R_2$
    \item $R_3 \rightarrow R_3 - 4R_2$
    \item $R_3 \times (-\frac{1}{10})$
    \item $R_1 \rightarrow R_1 - 3R_3$
  \end{itemize}
  To obtain:
  $$
    \begin{bmatrix}
      1 & 0 & 0 & | & \frac{3}{2}  & -\frac{11}{10} & -\frac{6}{5} \\
      0 & 1 & 0 & | & -1           & 1              & 1            \\
      0 & 0 & 1 & | & -\frac{1}{2} & \frac{7}{10}   & \frac{2}{5}
    \end{bmatrix}
  $$
  That is:
  $$A^{-1} = \begin{bmatrix}
      \frac{3}{2}  & -\frac{11}{10} & -\frac{6}{5} \\
      -1           & 1              & 1            \\
      -\frac{1}{2} & \frac{7}{10}   & \frac{2}{5}
    \end{bmatrix}$$
  It is easily checked that $AA^{-1} = I_3$.
\end{examplebox}
\pagebreak
\section{Spanning sets, bases and dimensions}
\subsection{Vector Spaces}
A \textbf{vector space} $\mathbf{V}$ over $\mathbb{F}$ is a non empty set of objects equipped with an addition operation and whose elements can be multiplied by scalars in $\mathbb{F}$, subject to the following axioms:
\begin{enumerate}
  \item $u+v = v+u, \quad \forall \; u,v \in \mathbf{V} $
  \item $(u+v) +w = u+(v+w), \quad \forall \; u,v,w \in \mathbf{V} $
  \item $\exists \; 0_\mathbf{V}$ , so that $0_v + v = v, \quad \forall \; v \in \mathbf{V} $
  \item $\exists \; -v \in \mathbf{V}$, so that $v + (-v) = 0_\mathbf{V}, \quad \forall \; v \in \mathbf{V} $
  \item $\alpha(\beta v) = \alpha\beta(v), \quad \forall \; \alpha,\beta \in \mathbb{F}, v \in \mathbf{V}$
  \item $(\alpha + \beta)v = \alpha v + \beta v, \quad \forall \; \alpha,\beta \in \mathbb{F}, v \in \mathbf{V}$
  \item $1v = v, \quad \forall \; v \in \mathbf{V}$
\end{enumerate}
In the definitions axioms above, the field $\mathbb{F}$ can be replaced with any other field, such as $\mathbb{R}$ or $\mathbb{C}$.
\subsubsection*{Examples of vector spaces over $\mathbb{R}$}
\begin{itemize}
  \item The space $M_{m \times n}(\mathbb{R})$ of $m \times n$ with real entries.
  \item The space of all polynomials with real coefficients
  \item The set of complex numbers is a vector space over $\mathbb{R}$.
\end{itemize}
Consider the space $\mathbf{V}$ consisting of all \textbf{symmetric} $2 \times 2$ matrices in $M_2(\mathbb{R})$ with \textbf{trace zero}.
\begin{itemize}
  \item \textbf{Trace zero} means that the sum of the diagonal elements is zero.
  \item \textbf{Symmetric} means that the matrix is equal to its transpose.
\end{itemize}
So a matrix of trace zero has the form:
$$
  \begin{bmatrix}
    a & b  \\
    b & -a
  \end{bmatrix}
  \quad \text{where} \; a,b \in \mathbb{R}
$$
Since it takes two real number to specify an element of $\mathbb{V}$, this is another example of a 2-dimensional vector.
\subsection{Subspaces}
\begin{definitionbox}{Vector Subspaces}{}
  Let $\mathbf{V}$ be a vector space over a field $\mathbb{F}$. A subset $\mathbf{U}$ is a \textbf{subspace} of $\mathbf{V}$ if $\mathbf{U}$ is itself a a vector space over $\mathbb{F}$, under  the addition and scalar multiplication operations defined on $\mathbf{V}$.\\[2ex]

  Two things need to be checked to confirm that $U \subseteq V$ is a subspace:
  \begin{enumerate}
    \item $\mathbb{U}$ is \textbf{closed} under the addition in $\mathbf{V}$, i.e. $u_1 + u_2 \in \mathbf{U}$ for all $u_1,u_2 \in \mathbf{U}$.
    \item $\mathbb{U}$ is \textbf{closed} under scalar multiplication, i.e. $\alpha u \in \mathbb{U}$, whenever $u \in \mathbb{U}$ and $\alpha \in \mathbb{F}$.
  \end{enumerate}
\end{definitionbox}
\subsubsection*{Examples of subspaces}
\begin{enumerate}
  \item Let $\mathbb{Q}[x]$ be the set of all polynomials with rational coefficients. Let $P_2 \subseteq \mathbb{Q}[x]$ be the set of all polynomials of degree at most 2. This means $P_2 = \{a_2x^2 + a_1x + a_0: a_0, a_1, a_2 \in \mathbf{Q}\}$ Then $P_2$ is a vector subspace of $\mathbb{Q}[x]$. If $f(x)$ and $g(x)$ are rational polynomials of degree at most 2, then also is $f(x) + g(x)$ and $\alpha f(x)$, where $\alpha \in \mathbb{Q}$.
  \item The set $\mathbb{C}$ is a vector space over the set of real numbers. Within $\mathbb{C}$, the subset $\mathbb{R}$ is an example of a vector subspace over $\mathbb{R}.$ An example of a subset of $\mathbb{C}$ that is not a real vector subset is the unit circle S in the complex plane- this is the set of complex numbers of modulus 1, it consists of all complex numbers of the form a+bi, where $a^2+b^2=1.$ This is closed neither under addition nor multiplication by real scalars.
  \item The Cartesian plane ($\mathbb{R}^2$) is a real vector space. Within $\mathbb{R}^2$, let $U=\{(a,b):a > 0,b >0\}$. Then $\mathbf{U}$ is closed under addition and under multiplication by positive scalars. It is not a vector subspace of $\mathbb{R}^2$,because it is not closed under multiplication by negative scalars
  \item Let $v$ be a fixed non-zero vector $\in \mathbb{R}^3$ and let $v^{\perp} = \{u \in \mathbb{R}^3: u^Tv = 0\}$. Then $v^{\perp}$ is not empty since $0 \in v^{\perp}$.Suppose $u_1, u_2 \in v^{\perp}$. If $u \in v^{\perp}$ and $\alpha \in \mathbb{R}$, then $(\alpha u)^T v = \alpha u^T v = 0 = \alpha 0 = 0$. Hence $v^{\perp}$ is closed under scalar multiplication. Thus $v^{\perp}$ is a vector subspace of $\mathbb{R}^3$. Note that $v^{\perp}$ is not all $\mathbb{R}^3$, since $v \notin v^{\perp}$.
\end{enumerate}
\subsection{Span of a set of vectors}
\begin{definitionbox}{Span}{}
  Let $\mathbf{V}$ be a vector space over a field $\mathbb{F}$, and let $S$ be a non empty subset of $\mathbf{V}$. \\[2ex]
  The $\mathbb{F}$-linear span, commonly called the \textbf{span} of $S$, denoted $\langle S \rangle$, is the set of all $\mathbb{F}$-linear combinations of the elements of $S \in \mathbf{V}$. \\[2ex]
  If $S=V$, then $S$ is called a spanning set of $V$; meaning that every element of $\mathbf{V}$ is a linear combination of the elements of $S$.
\end{definitionbox}
For a subset $S$ of a $\mathbb{F}$-vector space $\mathbf{V}$, the sum of any two linear combinations of $S$ is an element of $S$, and any scalar multiple of a linear combination of $S$ is also an element of $S$; hence the following lemma:
\begin{lemmabox}{}{}
  For any subset, $S$, of a vector space, $\mathbf{V}$, the span, $\langle S \rangle$, is a subspace of $\mathbf{V}$.
\end{lemmabox}
\subsubsection*{Examples}
\begin{itemize}
  \item \textbf{Polynomials over} $\mathbb{Q}$ \\
        $Q[x]$ is the set of all polynomials with rational coefficients, and $P_{2}\subset Q[x]$ consists of polynomials of degree at most 2.
        If $S=\{x^{2}+1,\, x+1\}$, then
        $$
          \langle S\rangle \;=\;\{\,a(x^2 + 1) \;+\; b(x+1)\;:\;a,b \in \mathbb{Q}\}.
        $$
        All members of $\langle S\rangle$ are degree-$\le 2$ polynomials with constant term equal to the sum of the $x$- and $x^2$-coefficients.
        For instance, $x^2+2x+3 \in \langle S\rangle$ but $x^2+2x+4 \notin \langle S\rangle$.
        Since $\langle S\rangle$ does not include all degree-$\le 2$ polynomials in $P_{2}$, $S$ is not a spanning set for $P_{2}$ over $\mathbb{Q}$.
  \item \textbf{Column vectors in} $\mathbb{R}^2$ \\
        Let
        $$
          S=\{(3,1),\,(2,1),\,(1,-1)\}.
        $$
        Any vector $(a,b)\in \mathbb{R}^2$ can be written as a linear combination of these three vectors in more than one way.
        However, $(1,-1)$ itself is a linear combination of $(3,1)$ and $(2,1)$, so it is not necessary to span $\mathbb{R}^2$.
        Hence $S$ has redundant elements and is not a minimal spanning set of $\mathbb{R}^2$.
\end{itemize}
The second example above motivates the following lemma:
\begin{lemmabox}{}{}
  Suppose that $S_1 \subset S$, where $S \subseteq \mathbf{V}$, then
  $$\langle S_1 \rangle \subseteq \langle S \rangle$$
  if and only if every element of $S \backslash S_1$ is a linear combination of the elements of $S_1$.
\end{lemmabox}
\begin{definitionbox}{}{}
  \begin{itemize}
    \item \textbf{Finite dimensional}: A vector space that has a finite spanning set
    \item \textbf{Infinite dimensional}: A vector space that has an infinite spanning set
  \end{itemize}
\end{definitionbox}
\subsubsection*{Example of infinite dimensional vector space}
\begin{itemize}
  \item The vector space $\mathbb{R}[x]$ of all polynomials with real coefficients is infinite dimensional. To see this let $S$ be a finite subset of $\mathbb{R}[x]$ and let $x^k$ be the highest power of $x $ in $S$. Then $x^{k+1} \notin \langle S \rangle$ since $x^{k+1}$ cannot be expressed as a linear combination of the elements of $S$.
  \item The set of $\mathbb{R}$ is infinite dimensional as a vector space over the field, $\mathbb{Q}$, of rational numbers.
\end{itemize}
\subsection{Linear independence}
\begin{definitionbox}{}{}
  Let $S \subseteq \mathbf{V}$ with at least two elements. \\[2ex]
  Then $S$ is linearly independent if \textbf{no element of $S$ can be expressed as a linear combination of the other elements of $S$}. \\[2ex]
  Equivalently, if no element of $S$ belongs to the span of the other elements of $S$.
\end{definitionbox}
It follows, a subset consisting of a single element is linearly independent if and only if that element is non-zero. The definition above takes a lot of work to check for large sets, the following definition is often more useful:
\begin{definitionbox}{}{}
  Let $S$ be a non-empty subset of $\mathbf{V}$.\\[2ex]
  Then $S$ is \textbf{linearly independent} if the only linear combination of the elements of $S$ that equals zero is to take all the coefficients to be zero.
\end{definitionbox}
\subsubsection*{Equivalence of the two definitions}
Let $S = \{v_1, \dots, v_k\}$ and suppose $v_1 \in \langle v_2, \dots v_k \rangle$. Then:
$$
  v_1 = \alpha_2 v_2 + \cdots + \alpha_k v_k \quad \Rightarrow \quad
  0 = - v_1 + \alpha_2 v_2 + \cdots + \alpha_k v_k
$$
is an expression for the zero vector as a linear combination of elements of $S$, whose coefficients are not all zero. On the other hand suppose:
$$
  0 = c_1v_1 + c_2v_2 + \cdots + c_kv_k
$$
where not all $c_i = 0$ Then:
$$
  v_1 = -\frac{c_2}{c_1}v_2 - \cdots - \frac{c_k}{c_1}v_k \quad \Rightarrow \quad
  v_1 \in \langle v_2, \dots, v_k \rangle
$$
\begin{examplebox}{}{}
  In $\mathbb{R}^3$, let $S = \{[1,2,-1], [-2, 3, 2], [-3,8,3]\}$. Show that $S$ is linearly independent.
  \\[2ex] \rule{\textwidth}{1px} \\
  To determine if $S$ is linearly independent, we need to investigate whether the system of equations has solutions other than $(x,y,z) = (0,0,0)$:
  $$ x \begin{bmatrix}
      1 \\
      2 \\
      -1
    \end{bmatrix}
    + y \begin{bmatrix}
      -2 \\
      3  \\
      2
    \end{bmatrix}
    + z \begin{bmatrix}
      -3 \\
      8  \\
      3
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 \\
      0 \\
      0
    \end{bmatrix}
    \quad\Longrightarrow \quad
    \begin{bmatrix}
      1  & -2 & -3 & | & 0 \\
      2  & 3  & 8  & | & 0 \\
      -1 & 2  & 3  & | & 0
    \end{bmatrix}
  $$
  Reducing it to its RREF we get:
  $$
    \begin{bmatrix}
      1 & 0 & 1 & | & 0 \\
      0 & 1 & 2 & | & 0 \\
      0 & 0 & 0 & | & 0
    \end{bmatrix}
    \quad\Longrightarrow\quad
    \begin{array}{l}
      x  + t = 0 \\
      y + 2t = 0 \\
      z + t = 0
    \end{array}
    \quad\Longrightarrow\quad
    (x,y,z) = (-t, -2t, t)
  $$
  Setting $t = 1$ gives:
  $$
    -1 \begin{bmatrix}
      1 \\
      2 \\
      -1
    \end{bmatrix}
    -2 \begin{bmatrix}
      -2 \\
      3  \\
      2
    \end{bmatrix}
    +1 \begin{bmatrix}
      -3 \\
      8  \\
      3
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 \\
      0 \\
      0
    \end{bmatrix}
  $$
\end{examplebox}
\end{document}