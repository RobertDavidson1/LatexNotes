\documentclass[a4paper, 9pt]{extarticle}
\input{../../preamble.sty}
\input{../../macros.sty}

\title{
\textbf{MA283: Linear Algebra} \\ 
}

\usepackage{geometry}
 \geometry{
 a4paper,
 bottom=22mm,
 }

\author{
  70\% Exam\\
30\% Continuous Assessment (Homework) \\
10\% Optional Project (Bonus)\\ [2ex]
Robert Davidson
}     
\date{}       % Optional: Add date if desired

\begin{document}

\maketitle
\pagebreak

\tableofcontents
\pagebreak
\section{Review of Matrix Algebra}
\subsection*{Matrix Addition}
If a matrix has $m$ rows and $n$ columns, we say it is $m \times n$.\textbf{Two matrices can only be added if they have the same size.}. In this case, we just add the entries in each position. \\[2ex]
The $m \times n$ \textbf{zero} matrix is a matrix with all entries equal to $0$. It is the \textbf{Identity element} for matrix addition (adding it to any matrix does not change the matrix)
\subsection*{Matrix Multiplication by a Scalar}
This simply means multiplying each entry of the matrix by the scalar. For example:
$$
  \alpha \begin{bmatrix}
    1 & 2 \\
    3 & 4 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \alpha  & 2\alpha \\
    3\alpha & 4\alpha \\
  \end{bmatrix}
$$

\textbf{Remark}: Now that we have addition and scalar multiplication, we can subtract matrices ($A - B = A + (-1)B$), provided they are the same size.

\subsection*{Vector Space}
With these operations of addition and scalar multiplication, the set of
$m \times n$ matrices is a vector space. A \textbf{vector space} algebraic structure whose elements can be added, subtracted and multiplied by scalars.

\subsection*{Linear Combinations}
\begin{definitionbox}{Linear Combinations}{}
  Suppose $v_1, v_2, \dots, v_k$ are elements that can be added together and multiplied by scalars. \\[2ex]
  A Linear Combination of $v_1, v_2, \dots, v_k$ is an expression of the form:
  $$
    \alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_k v_k
  $$
  where $a_i \in \mathbb{R}$ are scalars, called \textbf{coefficients}.
\end{definitionbox}
\subsection*{Matrix-Vector Multiplication}
\begin{definitionbox}{}{}
  Let $A$ be a $m \times n$ matrix, and $\textbf{v}$ be a column vector with $n$ entries ($n \times 1$ matrix). \\[2ex]
  Then the matrix vector product $Av$ is the column vector, with $m$ entries, obtained by taking the linear combination of the columns of $A$ with the entries of $\textbf{v}$ as coefficients.
  \\ \rule{\textwidth}{1px} \\
  $$
    \begin{bmatrix}
      -1 & 2 & 4 \\
      0  & 1 & 3
    \end{bmatrix}
    \begin{bmatrix}
      7 \\
      6 \\
      9
    \end{bmatrix}
    =
    7 \begin{bmatrix}
      -1 \\
      0
    \end{bmatrix}
    + 6 \begin{bmatrix}
      2 \\
      1
    \end{bmatrix}
    + 9 \begin{bmatrix}
      4 \\
      3
    \end{bmatrix}
    =
    \begin{bmatrix}
      41 \\
      33
    \end{bmatrix}.
  $$
  \rule{\textwidth}{1px} \\[2ex]
  \textbf{Remark:} $Av$, if defined, has the same number of rows as $A$ and the same number of columns as $\textbf{v}$.
\end{definitionbox}

\subsection*{Matrix-Matrix Multiplication}
\begin{definitionbox}{}{}
  Let $A$ and $B$ be matrices of size $m \times p$ and $p \times n$, respectively.Write $v_1, \ldots v_n$ for the columns of $B$. Then the product $AB$ is the $m \times n$ matrices whose columns are $Av_1 , \ldots, Av_n$.
  \\ \rule{\textwidth}{1px} \\
  The entry at row $i$ and column $j$ of the matrix $A$ is given by $A_{ij}$. The entry in the $i,j$ position of the product $AB$ is the $i$th entry of the vector $Av_j$, where the vector $v_j$ is the $j$th column of $B$. In other words, the entry in the $i,j$ position of the product $AB$ is given by:
  $$
    (AB)_{ij} = A_{i1}B_{1j} + A_{i2}B_{2j} + \ldots + A_{ip}B_{pj} = \sum_{k=1}^p A_{ik}B_{kj}
  $$
\end{definitionbox}
\begin{definitionbox}{}{}
  If $A$ is $m \times p$ with rows $u_1, \ldots, u_m$ and $B$ is $p \times n$ with columns $v_1, \ldots, v_n$, then the product $AB$ is:
  $$AB =
    \begin{bmatrix}
      u_1 \cdot v_1 & u_1 \cdot v_2 & \ldots & u_1 \cdot v_n \\
      u_2 \cdot v_1 & u_2 \cdot v_2 & \ldots & u_2 \cdot v_n \\
      \vdots        & \vdots        & \ddots & \vdots        \\
      u_m \cdot v_1 & u_m \cdot v_2 & \ldots & u_m \cdot v_n
    \end{bmatrix}
  $$
  \rule {\textwidth}{1px} \\[2ex]
  \textbf{Example}:
  $$
    A =
    \begin{bmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6
    \end{bmatrix}
    \quad
    B =
    \begin{bmatrix}
      7  & 8  \\
      9  & 10 \\
      11 & 12
    \end{bmatrix}
    \quad
    AB =
    \begin{bmatrix}
      1 \cdot 7 + 2 \cdot 9 + 3 \cdot 11 & 1 \cdot 8 + 2 \cdot 10 + 3 \cdot 12 \\
      4 \cdot 7 + 5 \cdot 9 + 6 \cdot 11 & 4 \cdot 8 + 5 \cdot 10 + 6 \cdot 12
    \end{bmatrix}
    =
    \begin{bmatrix}
      58  & 64  \\
      139 & 154
    \end{bmatrix}
  $$
\end{definitionbox}
For matrices $A$ and $B$, the products $AB$ and $BA$ are generally not
equal, even if they are both defined and even if both have the same
size.
\subsection*{Linear Transformations}
\begin{definitionbox}{}{}
  Let $m$ and $n$ be positive integers. \\[2ex]
  A \textbf{linear transformation} $T$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ is a function $T:\mathbb{R}^n \to \mathbb{R}^m$ that satisfies:
  \begin{itemize}
    \item $T(\textbf{u} + \textbf{v}) = T(\textbf{u}) + T(\textbf{v})$ for all $\textbf{u}, \textbf{v} \in \mathbb{R}^n$
    \item $T(\lambda\textbf{u}) = \lambda T(\textbf{u})$ for all $\textbf{u} \in \mathbb{R}^n$ and $\lambda \in \mathbb{R}$
  \end{itemize}
\end{definitionbox}
\subsection*{Matrix of a Linear Transformation}
Suppose $T:\mathbb{R}^3 \to \mathbb{R}^2$ is the linear transformation:
$$
  T\begin{bmatrix}
    1 \\
    0 \\
    0 \\
  \end{bmatrix}
  = \begin{bmatrix}
    2 \\
    3 \\
  \end{bmatrix}
  \quad
  T\begin{bmatrix}
    0 \\
    1 \\
    0 \\
  \end{bmatrix}
  = \begin{bmatrix}
    1 \\
    4 \\
  \end{bmatrix}
  \quad
  T\begin{bmatrix}
    0 \\
    0 \\
    1 \\
  \end{bmatrix}
  = \begin{bmatrix}
    -6 \\
    7  \\
  \end{bmatrix}
$$
Then for the vector in $\mathbb{R}^3$ with entries $a,b,c$:
$$
  T\begin{bmatrix}
    a \\
    b \\
    c \\
  \end{bmatrix}
  = aT\begin{bmatrix}
    1 \\
    0 \\
    0 \\
  \end{bmatrix}
  + bT\begin{bmatrix}
    0 \\
    1 \\
    0 \\
  \end{bmatrix}
  + cT\begin{bmatrix}
    0 \\
    0 \\
    1 \\
  \end{bmatrix}
  = \begin{bmatrix}
    2  & 1 & -6 \\
    -3 & 4 & 7
  \end{bmatrix}
  \begin{bmatrix}
    a \\
    b \\
    c
  \end{bmatrix}
$$
Where the $2\times 3$ matrix $M_T$ is called the \textbf{standard matrix} of A.
A linear transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ can be completely represented by an $m \times n$ matrix $M_T$. \\[2ex]
\textbf{Understanding the Matrix Representation}
\begin{itemize}
  \item The columns of matrix $M_T$ are the images of the standard basis vectors $e_1, e_2, \ldots, e_n$ under $T$.
  \item For any vector $v \in \mathbb{R}^n$, we calculate $T(v)$ by multiplying: $M_T \cdot v$.
  \item Therefore, matrix-vector multiplication is simply evaluating a linear transformation.
\end{itemize}
\textbf{Correspondence:} Any $m \times n$ matrix $A$ defines a linear transformation $T_A: \mathbb{R}^n \rightarrow \mathbb{R}^m$ by: $T_A(v) = Av$. Linear transformations include rotations, reflections and scaling \\[2ex]
\textbf{Efficiency of Representation:} A remarkable property of linear transformations is their information efficiency:
\begin{itemize}
  \item To completely define $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$, we need only $mn$ values.
  \item These values are the coordinates of the $n$ transformed basis vectors in $\mathbb{R}^m$.
  \item This differs fundamentally from general continuous functions $f: \mathbb{R} \rightarrow \mathbb{R}$, which cannot be fully determined by their values at finitely many points.
\end{itemize}
\pagebreak
\subsection*{Matrix multiplication is composition}
Suppose that $T:\mathbb{R}^n \to \mathbb{R}^p$ and $S:\mathbb{R}^p \to \mathbb{R}^m$ are linear transformations. Then the composition $S \circ T: \mathbb{R}^n \to \mathbb{R}^m$ is also a linear transformation from $\mathbb{R}^n$ to $\mathbb{R}^m$ defined for $\mathbf{v} \in \mathbb{R}^n$ by:
$$
  S \circ T(\mathbf{v}) = S(T(\mathbf{v}))
$$
To see how that the $m \times n$ matrix $M_{S\circ T}$ depends on the matrix $M_S (m \times p)$ and $M_T (p \times n)$ we look at the definition of $M_{S\circ T}$:
\begin{itemize}
  \item The first column has coordinates $S\circ T(e_1) = S(T(e_1))$
  \item $T(e_1)$ is first column of $M_T$
  \item Then $S(T(e_1))$ is the matrix-vector product $M_S \cdot M_T(e_1)$
  \item Same for all other columns $\Longrightarrow$ $M_{S\circ T} = M_S \cdot M_T$
\end{itemize}
Thus, we conclude $\textbf{matrix multiplication is composition of linear transformations}$.
\pagebreak
\section{Systems of linear equations}
\subsection{Linear equations and Solution Sets}
A linear equation in the variables $x$ and
$y$ is an equation of the form
\begin{equation*}
  2x + y = 3
\end{equation*}
If we replace $x$ and $y$ with some numbers, the statement \textbf{becomes true or false}.

\begin{definitionbox}{Solution to a linear equation}{}
  A pair, $(x_0, y_0) \in \mathbb{R}$, is a solution to an linear equation if setting $x = x_0$ and $y = y_0$ \textbf{makes the equation true.}
\end{definitionbox}

\begin{definitionbox}{Solution set}{}
  The \textbf{solution set} is the set of all solutions to a linear equation.
  $$a_1X_1 + a_2X_2 + \ldots + a_nX_n = b \quad \text{where} \; a_i, b \in \mathbb{R}$$
  is an \textbf{affine hyperplane} in $\mathbb{R}^n$; geometrically resembles a copy of $\mathbb{R}^{n-1}$ inside $\mathbb{R}^n$.
\end{definitionbox}
\subsubsection{Interpreting Linear Systems as Matrix Equations}
$$
  \begin{array}{ccccccc}
    x  & + & 2y & - & z  & = & 5 \\
    3x & + & y  & - & 2z & = & 9 \\
    -x & + & 4y & + & 2z & = & 0
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1  & 2 & -1 \\
    3  & 1 & -2 \\
    -1 & 4 & 2
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y \\
    z
  \end{bmatrix}
  =
  \begin{bmatrix}
    5 \\
    9 \\
    0
  \end{bmatrix}
$$
\subsection{Elementary Row Operations}
To solve a system of linear equations we associate an \textbf{augmented matrix} to the system of equations. For example:
$$
  \begin{array}
    {ccccccc}x & + & 2y & - & z  & = & 5 \\
    3x         & + & y  & - & 2z & = & 9 \\
    -x         & + & 4y & + & 2z & = & 0
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1  & 2 & -1 & | & 5 \\
    3  & 1 & -2 & | & 9 \\
    -1 & 4 & 2  & | & 0
  \end{bmatrix}
$$
To solve, we can perform the following \textbf{Elementary Row Operations (EROs)}:
\begin{enumerate}
  \item Multiply a row by a non-zero constant.
  \item Add a multiple of one row to another row.
  \item Swap two rows.
\end{enumerate}
The goal of these operations is to transform the augmented matrix into \textbf{row echelon form} (REF) or \textbf{reduced row echelon form} (RREF).

\subsubsection{REF and Strategy}
\begin{minipage}{0.8\textwidth}
  We say a matrix is in \textbf{row echelon form} (REF) if:
  \begin{itemize}
    \item The first non zero entry in each row is a 1 (called the \textbf{leading 1}).
    \item If a column has a leading 1, then all entries below it are 0.
    \item The leading 1 in each row is to the right of the leading 1 in the previous row.
    \item All rows of 0s are at the bottom of the matrix.
  \end{itemize}
\end{minipage}
\begin{minipage}{0.2\textwidth}
  \begin{center}
    $$
      \begin{bmatrix}
        1 & 2 & -1 & | & 3  \\
        0 & 1 & 2  & | & -1 \\
        0 & 0 & 1  & | & -1
      \end{bmatrix}
    $$
    \emph{Example of REF}
  \end{center}


\end{minipage} \\[2ex]
We have produced a new system of equations. This is easily solved by back substitution.

\begin{conceptbox}{Stategy for Obtaining REF}{}
  \begin{itemize}
    \item Get a 1 as the top left entry
    \item Use this 1 to clear the entries below it
    \item Move to the next column and repeat
    \item Continue until all leading 1s are in place
    \item Use back substitution to solve the system
  \end{itemize}
\end{conceptbox}
\subsubsection{Row Reduced Echelon Form}
\begin{minipage}{0.8\textwidth}
  A matrix is in \textbf{reduced row echelon form} (RREF) if:
  \begin{itemize}
    \item It is in REF
    \item The leading 1 in each row is the only non-zero entry in its column.
  \end{itemize}
\end{minipage}
\begin{minipage}{0.2\textwidth}
  \begin{center}
    $$
      \begin{bmatrix}
        1 & 0 & 0 & | & 2  \\
        0 & 1 & 0 & | & -1 \\
        0 & 0 & 1 & | & -1
      \end{bmatrix}
    $$
    \emph{Example of RREF}
  \end{center}
\end{minipage}

\subsection{Leading variables and free variables}
We'll start by an example:
$$
  \begin{array}{ccccccccc}
    x_1  & - & x_2  & - & x_3  & + & 2x_4 & = & 0 \\
    2x_1 & + & x_2  & - & x_3  & + & 2x_4 & = & 8 \\
    x_1  & - & 3x_2 & + & 2x_3 & + & 7x_4 & = & 2
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1 & -1 & -1 & 2 & | & 0 \\
    2 & 1  & -1 & 2 & | & 8 \\
    1 & -3 & 2  & 7 & | & 2
  \end{bmatrix}
$$
Solving this system of equations, we get:
$$
  \text{RREF:}\quad
  \begin{bmatrix}
    1 & 0 & 0 & 2  & | & 4 \\
    0 & 1 & 0 & -1 & | & 2 \\
    0 & 0 & 0 & 1  & | & 2
  \end{bmatrix}
  \quad \Rightarrow \quad
  \begin{array}{ccccc}
    x_1 & + & 2x_4 & = & 4 \quad \\
    x_2 & - & x_4  & = & 2 \quad \\
    x_3 & + & x_4  & = & 2 \quad
  \end{array}
  \quad \Rightarrow \quad
  \begin{array}{ccc}
    x_1 & = & 4 - 2x_4 \\
    x_2 & = & 2 + x_4  \\
    x_3 & = & 2 - x_4
  \end{array}
$$
This RREF tells us how the \textbf{leading variables} ($x_1, x_2, x_3$) depend on the \textbf{free variable} ($x_4$). The free variable can take any value in $\mathbb{R}$. We write the solution set as:
$$x_1 = 4 - 2t, \quad x_2 = 2 + t, \quad x_3 = 2 - t, \quad x_4 = t \quad \text{where} \; t \in \mathbb{R}$$
$$(x_1, x_2, x_3, x_4) = (4-2t, 2+t, 2-t, t); \quad t \in \mathbb{R}$$
\begin{definitionbox}{Leading and Free Variables}{}
  \begin{itemize}
    \item  \textbf{Leading variable} : A variable whose columns in the RREF contain a leading 1
    \item \textbf{Free variable} : A variable whose columns in the RREF do not contain a leading 1
  \end{itemize}
\end{definitionbox}

\subsection{Consistent and Inconsistent Systems}
Consider the following system of equations:
$$
  \begin{array}{ccccccc}
    3x & + & 2y & - & 5z & = & 4 \\
    x  & + & y  & - & 2z & = & 1 \\
    5x & + & 3y & - & 8z & = & 6
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    3 & 2 & -5 & | & 4 \\
    1 & 1 & -2 & | & 1 \\
    5 & 3 & -8 & | & 6
  \end{bmatrix}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1 & 1 & -2 & | & 1 \\
    0 & 1 & -1 & | & 1 \\
    0 & 0 & 0  & | & 1
  \end{bmatrix} \quad \text{(REF)}
$$
We can see the last row of the REF is:
$$
  0x + 0y + 0z = 1
$$
This equation clearly has no solution, and hence the system has no solutions. We say the system is \textbf{inconsistent}. Alternatively, we say the system is \textbf{consistent} if it has at least one solution.
\subsection{Possible Outcomes when solving a system of equations}
\begin{itemize}
  \item The system may be \textbf{inconsistent} (no solutions) - i.e:
        $$ [0 \; 0 \; \dots \; 0 \; | \; a] \quad a \neq 0$$
  \item The system may be \textbf{consistent} which occurs if:
        \begin{itemize}
          \item \textbf{Unique Solutions} each column (aside from the rightmost) contains a single leading 1. - i.e:
                $$
                  \begin{bmatrix}
                    1 & 0 & 0 & | & 4  \\
                    0 & 1 & 0 & | & 3  \\
                    0 & 0 & 1 & | & -2
                  \end{bmatrix}
                $$

          \item \textbf{Infinitely many solutions} at least one variable does not appear as a leading 1 in any row, making it a free variable - i.e:
                $$
                  \begin{bmatrix}
                    1 & 2 & -1 & | & 3  \\
                    0 & 0 & 1  & | & -2 \\
                    0 & 0 & 0  & | & 0
                  \end{bmatrix}
                $$
        \end{itemize}
\end{itemize}

\subsection{Elementary Row Operations as Matrix Transformations}
Elementary row operations may be interpreted as \textbf{matrix multiplication}. To see this, first we introduce the \textbf{Identity matrix:}
\vspace{-2ex}
$$I_3 = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix}$$
The $I_m$ Identity matrix is an $m \times m$ matrix with 1s on the diagonal and 0s elsewhere. We also introduce the $E_{i,j}$ matrix which has $1$ in the $(i,j)$ position and $0$s elsewhere. For example:
$$
  E_{1,2} = \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
$$
Then:
\vspace{-2ex}
$$I_3 + 4E_{1,2} = \begin{bmatrix}
    1 & 4 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix}$$
Performing a row operation on $A$ is the same as multiplying $A$ by an appropriate matrix $E$ on the left.
These matrices are called \textbf{elementary matrices}. They are \textbf{always invertible,} and \textbf{their inverses are also elementary matrices}. The statement:
$$
  "\emph{every matrix can be reduced to RREF through EROs}"$$
is equivalent to saying that
\begin{align*}
  "\emph{for every matrix $A$ with $m$ rows, there exists a $m \times m$ matrix $B$} \\
  \emph{which is a product of elementary matrices such that $BA$ is in RREF.}"
\end{align*}
\vspace{-8ex}
\subsubsection{Multiplying a Row by a Non-Zero Scalar}
When multiplying row $i$ of matrix $A$ by a scalar $\alpha \neq 0$, we can use the matrix:
$$I_m + (\alpha - 1)E_{i,i}$$
This works because it modifies only the $(i,i)$ entry of the identity matrix to be $\alpha$ while keeping all other entries unchanged. When multiplied with $A$, it scales row $i$ by $\alpha$ and leaves all other rows intact.\\
\textbf{Example:} If $\alpha = 5$ and $i = 2$, then:
\vspace{-2ex}
$$
  I_3 + 4E_{2,2} = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 5 & 0 \\
    0 & 0 & 1
  \end{bmatrix}\
  \quad
  A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad \quad\quad
  (I_3 + 4E_{2,2})A = \begin{bmatrix}
    1  & 2  & 3  \\
    20 & 25 & 30 \\
    7  & 8  & 9
  \end{bmatrix}$$
\vspace{-4ex}
\subsubsection{Switching Two Rows}
To swap rows $i$ and $k$, we use:
$$S = I_m + E_{i,k} + E_{k,i} - E_{i,i} - E_{k,k}$$
This works by:
\begin{itemize}
  \item Removing the 1's at positions $(i,i)$ and $(k,k)$ from the identity matrix
  \item Adding 1's at positions $(i,k)$ and $(k,i)$
\end{itemize}
\vspace{-1ex}
\textbf{Example:} Swapping rows 1 and 3:
\vspace{-2ex}
$$  S = \begin{bmatrix}
    0 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 0
  \end{bmatrix}
  \quad
  A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad \quad\quad
  SA = \begin{bmatrix}
    7 & 8 & 9 \\
    4 & 5 & 6 \\
    1 & 2 & 3
  \end{bmatrix}$$
\vspace{-5ex}
\subsubsection{Adding a Multiple of One Row to Another}
To replace row $k$ with row $k + \alpha \times$ row $i$, use:
$$I_m + \alpha E_{k,i}$$
This adds $\alpha$ times row $i$ to row $k$ while leaving all other rows unchanged because:
\begin{itemize}
  \item For any row $j \neq k$, the corresponding row in this matrix is just the standard basis row
  \item Row $k$ becomes the sum of the standard basis row $k$ plus $\alpha$ times the standard basis row $i$
\end{itemize}
\textbf{Example:} Adding 3 times row 1 to row 2:
\vspace{-1ex}
$$ I_3 + 3E_{2,1} = \begin{bmatrix}
    1 & 0 & 0 \\
    3 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix} \quad A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad\quad\quad
  (I_3 + 3E_{2,1})A = \begin{bmatrix}
    1 & 2  & 3  \\
    7 & 11 & 15 \\
    7 & 8  & 9
  \end{bmatrix}$$

\begin{examplebox}{}{}
  Write the inverse of an elementary matrix and show it is an elementary matrix.
  \\ \rule{\textwidth}{1px} \\
  \textbf{Multiplying a row by a nonzero scalar:}
  \begin{itemize}
    \item \textbf{Operation:} Multiply row $i$ by $\alpha \neq 0$.
    \item \textbf{Elementary Matrix:} $E = I_m + (\alpha - 1)E_{i,i}$
    \item \textbf{Inverse:} To reverse the operation, multiply row $i$ by $1\backslash\alpha$. Hence, the inverse is
          $$
            E^{-1} = I_m + \left(\frac{1}{\alpha} - 1\right)E_{i,i}  =\; I_m + \frac{1-\alpha}{\alpha}\,E_{i,i}.
          $$
  \end{itemize}
  \textbf{Swapping two rows:}
  \begin{itemize}
    \item \textbf{Operation:} Swap rows $i$ and $k$.
    \item \textbf{Elementary Matrix:} $S = I_m - E_{i,i} - E_{k,k} + E_{i,k} + E_{k,i}$
    \item \textbf{Inverse:} Since swapping the same two rows twice returns them to their original positions,
          $$
            S^{-1} = S.
          $$
  \end{itemize}
  \textbf{Adding a multiple of one row to another:}
  \begin{itemize}
    \item \textbf{Operation:} Add $\alpha$ times row $i$ to row $k$.
    \item \textbf{Elementary Matrix:} $E = I_m + \alpha\,E_{k,i}$
    \item \textbf{Inverse:} To undo the operation, subtract $\alpha$ times row $i$ from row $k$. Therefore,
          $$
            E^{-1} = I_m - \alpha\,E_{k,i}.
          $$
  \end{itemize}
\end{examplebox}
\begin{examplebox}{}{}
  Prove that every invertible matrix in $M_n(\mathbb{R})$ is a product of elementary matrices.
  \\ \rule{\textwidth}{1px} \\
  Let $A$  be an invertible matrix in $M_n(\mathbb{R})$. Since $A$ is invertible, we can use Gaussian elimination to transform $A$ into the identity matrix $I_n$. \\[2ex]
  Let $E_1, E_2, \ldots, E_k$ be the elementary matrices corresponding to the row operations used in the elimination process. Then, we have:
  \begin{align*}
    \text{Multiplying a row by a scalar:}           & \quad I_n + (\alpha - 1)E_{i,i}                   \\
    \text{Swapping two rows:}                       & \quad I_n + E_{i,k} + E_{k,i} - E_{i,i} - E_{k,k} \\
    \text{Adding a multiple of one row to another:} & \quad I_n + \alpha\,E_{k,i}
  \end{align*}
  Applying these in sequence to $A$ gives:
  $$E_k \cdots E_2 E_1 A = I_n$$
  Since $E_k \cdots E_2 E_1 = I_n$, we can multiply both sides by $(E_k \cdots E_2 E_1)^{-1}$ on the left to obtain:
  $$A = (E_k \cdots E_2 E_1)^{-1} I_n = (E_k \cdots E_2 E_1)^{-1}$$
  Using the property:
  $$(E_k \cdots E_2 E_1)^{-1} = E_1^{-1} E_2^{-1} \cdots E_k^{-1},$$
  we can express $A$ as a product of elementary matrices:
  $$A = E_1^{-1} E_2^{-1} \cdots E_k^{-1}$$
  Since each $E_i$ is an elementary matrix, its inverse is also an elementary matrix. Therefore, $A$ can be expressed as a product of elementary matrices.
\end{examplebox}
\pagebreak
\subsection{EROs and Inverses}
Elementary Row Operations can be used to find the inverse of a square matrix.  Consider a square matrix $A \in M_n(\mathbb{F})$ (that is, an $n \times n$ matrix over a field $\mathbb{F}$). If $A$ is invertible, let
$$
  A^{-1}
  =
  \begin{bmatrix}
    |            & |            &        & |            \\
    \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \\
    |            & |            &        & |
  \end{bmatrix}
$$
be its inverse, where each $\mathbf{v}_i$ is the $i$th column of $A^{-1}$. By definition of the matrix inverse, we have
$$
  A \, A^{-1}
  =
  A \begin{bmatrix}
    \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n
  \end{bmatrix}
  =
  \begin{bmatrix}
    A \mathbf{v}_1 & A \mathbf{v}_2 & \cdots & A \mathbf{v}_n
  \end{bmatrix}
  =
  I_n,
$$
the $n \times n$ identity matrix. This implies that
$$
  A \mathbf{v}_i = \mathbf{e}_i, \quad \text{for each } i=1, 2, \ldots, n,
$$
where $\mathbf{e}_i$ is the $i$th column of $I_n$ (which has a 1 in the $i$th row and 0 everywhere else). In other words, each column $\mathbf{v}_i$ of $A^{-1}$ is the unique solution to the linear system
$$
  A \mathbf{v}_i = \mathbf{e}_i.
$$
To find $A^{-1}$ effectively, we form the augmented matrix $[A \mid I_n]$ and apply EROs to trasnform $A$ into $I_n$. When this is achieved, the augmented portion becomes $A^{-1}$. Thus, we have
$$
  \text{RREF}\bigl([\,A \mid I_n\,]\bigr)
  =
  \bigl[\, I_n \mid A^{-1} \bigr].
$$
\begin{examplebox}{}{}
  Find $A^{-1}$ if $A = \begin{bmatrix}
      3 & 4 & -1 \\
      1 & 0 & 3  \\
      2 & 5 & -4
    \end{bmatrix}$.
  \\[2ex] \rule{\textwidth}{1px} \\
  We form a $3 \times 6$ matrix $A' = [A \mid I_3]$:
  $$
    A' = \begin{bmatrix}
      3 & 4 & -1 & | & 1 & 0 & 0 \\
      1 & 0 & 3  & | & 0 & 1 & 0 \\
      2 & 5 & -4 & | & 0 & 0 & 1
    \end{bmatrix}
  $$
  We apply the following EROs to $A'$:
  \begin{itemize}
    \item $R_1 \leftrightarrow R_2$
    \item $R_2 \rightarrow R_2 - 3R_1$
    \item $R_3 \rightarrow R_3 - 2R_1$
    \item $R_3 \rightarrow R_3 + R-2$
    \item $R_3 \leftrightarrow R_2$
    \item $R_3 \rightarrow R_3 - 4R_2$
    \item $R_3 \times (-\frac{1}{10})$
    \item $R_1 \rightarrow R_1 - 3R_3$
  \end{itemize}
  To obtain:
  $$
    \begin{bmatrix}
      1 & 0 & 0 & | & \frac{3}{2}  & -\frac{11}{10} & -\frac{6}{5} \\
      0 & 1 & 0 & | & -1           & 1              & 1            \\
      0 & 0 & 1 & | & -\frac{1}{2} & \frac{7}{10}   & \frac{2}{5}
    \end{bmatrix}
  $$
  That is:
  $$A^{-1} = \begin{bmatrix}
      \frac{3}{2}  & -\frac{11}{10} & -\frac{6}{5} \\
      -1           & 1              & 1            \\
      -\frac{1}{2} & \frac{7}{10}   & \frac{2}{5}
    \end{bmatrix}$$
  It is easily checked that $AA^{-1} = I_3$.
\end{examplebox}
\pagebreak
% \section{Spanning sets, bases and dimensions}
% \subsection{Vector Spaces}
% A \textbf{vector space} $\mathbf{V}$ over $\mathbb{F}$ is a non empty set of objects equipped with an addition operation and whose elements can be multiplied by scalars in $\mathbb{F}$, subject to the following axioms:
% \begin{enumerate}
%   \item $u+v = v+u, \quad \forall \; u,v \in \mathbf{V} $
%   \item $(u+v) +w = u+(v+w), \quad \forall \; u,v,w \in \mathbf{V} $
%   \item $\exists \; 0_\mathbf{V}$ , so that $0_v + v = v, \quad \forall \; v \in \mathbf{V} $
%   \item $\exists \; -v \in \mathbf{V}$, so that $v + (-v) = 0_\mathbf{V}, \quad \forall \; v \in \mathbf{V} $
%   \item $\alpha(\beta v) = \alpha\beta(v), \quad \forall \; \alpha,\beta \in \mathbb{F}, v \in \mathbf{V}$
%   \item $(\alpha + \beta)v = \alpha v + \beta v, \quad \forall \; \alpha,\beta \in \mathbb{F}, v \in \mathbf{V}$
%   \item $1v = v, \quad \forall \; v \in \mathbf{V}$
% \end{enumerate}
% In the definitions axioms above, the field $\mathbb{F}$ can be replaced with any other field, such as $\mathbb{R}$ or $\mathbb{C}$.
% \subsubsection*{Examples of vector spaces over $\mathbb{R}$}
% \begin{itemize}
%   \item The space $M_{m \times n}(\mathbb{R})$ of $m \times n$ with real entries.
%   \item The space of all polynomials with real coefficients
%   \item The set of complex numbers is a vector space over $\mathbb{R}$.
% \end{itemize}
% Consider the space $\mathbf{V}$ consisting of all \textbf{symmetric} $2 \times 2$ matrices in $M_2(\mathbb{R})$ with \textbf{trace zero}.
% \begin{itemize}
%   \item \textbf{Trace zero} means that the sum of the diagonal elements is zero.
%   \item \textbf{Symmetric} means that the matrix is equal to its transpose.
% \end{itemize}
% So a matrix of trace zero has the form:
% $$
%   \begin{bmatrix}
%     a & b  \\
%     b & -a
%   \end{bmatrix}
%   \quad \text{where} \; a,b \in \mathbb{R}
% $$
% Since it takes two real number to specify an element of $\mathbb{V}$, this is another example of a 2-dimensional vector.
% \subsection{Subspaces}
% \begin{definitionbox}{Vector Subspaces}{}
%   Let $\mathbf{V}$ be a vector space over a field $\mathbb{F}$. A subset $\mathbf{U}$ is a \textbf{subspace} of $\mathbf{V}$ if $\mathbf{U}$ is itself a a vector space over $\mathbb{F}$, under  the addition and scalar multiplication operations defined on $\mathbf{V}$.\\[2ex]

%   Two things need to be checked to confirm that $U \subseteq V$ is a subspace:
%   \begin{enumerate}
%     \item $\mathbb{U}$ is \textbf{closed} under the addition in $\mathbf{V}$, i.e. $u_1 + u_2 \in \mathbf{U}$ for all $u_1,u_2 \in \mathbf{U}$.
%     \item $\mathbb{U}$ is \textbf{closed} under scalar multiplication, i.e. $\alpha u \in \mathbb{U}$, whenever $u \in \mathbb{U}$ and $\alpha \in \mathbb{F}$.
%   \end{enumerate}
% \end{definitionbox}
% \subsubsection*{Examples of subspaces}
% \begin{enumerate}
%   \item Let $\mathbb{Q}[x]$ be the set of all polynomials with rational coefficients. Let $P_2 \subseteq \mathbb{Q}[x]$ be the set of all polynomials of degree at most 2. This means $P_2 = \{a_2x^2 + a_1x + a_0: a_0, a_1, a_2 \in \mathbf{Q}\}$ Then $P_2$ is a vector subspace of $\mathbb{Q}[x]$. If $f(x)$ and $g(x)$ are rational polynomials of degree at most 2, then also is $f(x) + g(x)$ and $\alpha f(x)$, where $\alpha \in \mathbb{Q}$.
%   \item The set $\mathbb{C}$ is a vector space over the set of real numbers. Within $\mathbb{C}$, the subset $\mathbb{R}$ is an example of a vector subspace over $\mathbb{R}.$ An example of a subset of $\mathbb{C}$ that is not a real vector subset is the unit circle S in the complex plane- this is the set of complex numbers of modulus 1, it consists of all complex numbers of the form a+bi, where $a^2+b^2=1.$ This is closed neither under addition nor multiplication by real scalars.
%   \item The Cartesian plane ($\mathbb{R}^2$) is a real vector space. Within $\mathbb{R}^2$, let $U=\{(a,b):a > 0,b >0\}$. Then $\mathbf{U}$ is closed under addition and under multiplication by positive scalars. It is not a vector subspace of $\mathbb{R}^2$,because it is not closed under multiplication by negative scalars
%   \item Let $v$ be a fixed non-zero vector $\in \mathbb{R}^3$ and let $v^{\perp} = \{u \in \mathbb{R}^3: u^Tv = 0\}$. Then $v^{\perp}$ is not empty since $0 \in v^{\perp}$.Suppose $u_1, u_2 \in v^{\perp}$. If $u \in v^{\perp}$ and $\alpha \in \mathbb{R}$, then $(\alpha u)^T v = \alpha u^T v = 0 = \alpha 0 = 0$. Hence $v^{\perp}$ is closed under scalar multiplication. Thus $v^{\perp}$ is a vector subspace of $\mathbb{R}^3$. Note that $v^{\perp}$ is not all $\mathbb{R}^3$, since $v \notin v^{\perp}$.
% \end{enumerate}
% \subsection{Span of a set of vectors}
% \begin{definitionbox}{Span}{}
%   Let $\mathbf{V}$ be a vector space over a field $\mathbb{F}$, and let $S$ be a non empty subset of $\mathbf{V}$. \\[2ex]
%   The $\mathbb{F}$-linear span, commonly called the \textbf{span} of $S$, denoted $\langle S \rangle$, is the set of all $\mathbb{F}$-linear combinations of the elements of $S \in \mathbf{V}$. \\[2ex]
%   If $S=V$, then $S$ is called a spanning set of $V$; meaning that every element of $\mathbf{V}$ is a linear combination of the elements of $S$.
% \end{definitionbox}
% For a subset $S$ of a $\mathbb{F}$-vector space $\mathbf{V}$, the sum of any two linear combinations of $S$ is an element of $S$, and any scalar multiple of a linear combination of $S$ is also an element of $S$; hence the following lemma:
% \begin{lemmabox}{}{}
%   For any subset, $S$, of a vector space, $\mathbf{V}$, the span, $\langle S \rangle$, is a subspace of $\mathbf{V}$.
% \end{lemmabox}
% \subsubsection*{Examples}
% \begin{itemize}
%   \item \textbf{Polynomials over} $\mathbb{Q}$ \\
%         $Q[x]$ is the set of all polynomials with rational coefficients, and $P_{2}\subset Q[x]$ consists of polynomials of degree at most 2.
%         If $S=\{x^{2}+1,\, x+1\}$, then
%         $$
%           \langle S\rangle \;=\;\{\,a(x^2 + 1) \;+\; b(x+1)\;:\;a,b \in \mathbb{Q}\}.
%         $$
%         All members of $\langle S\rangle$ are degree-$\le 2$ polynomials with constant term equal to the sum of the $x$- and $x^2$-coefficients.
%         For instance, $x^2+2x+3 \in \langle S\rangle$ but $x^2+2x+4 \notin \langle S\rangle$.
%         Since $\langle S\rangle$ does not include all degree-$\le 2$ polynomials in $P_{2}$, $S$ is not a spanning set for $P_{2}$ over $\mathbb{Q}$.
%   \item \textbf{Column vectors in} $\mathbb{R}^2$ \\
%         Let
%         $$
%           S=\{(3,1),\,(2,1),\,(1,-1)\}.
%         $$
%         Any vector $(a,b)\in \mathbb{R}^2$ can be written as a linear combination of these three vectors in more than one way.
%         However, $(1,-1)$ itself is a linear combination of $(3,1)$ and $(2,1)$, so it is not necessary to span $\mathbb{R}^2$.
%         Hence $S$ has redundant elements and is not a minimal spanning set of $\mathbb{R}^2$.
% \end{itemize}
% The second example above motivates the following lemma:
% \begin{lemmabox}{}{}
%   Suppose that $S_1 \subset S$, where $S \subseteq \mathbf{V}$, then
%   $$\langle S_1 \rangle \subseteq \langle S \rangle$$
%   if and only if every element of $S \backslash S_1$ is a linear combination of the elements of $S_1$.
% \end{lemmabox}
% \begin{definitionbox}{}{}
%   \begin{itemize}
%     \item \textbf{Finite dimensional}: A vector space that has a finite spanning set
%     \item \textbf{Infinite dimensional}: A vector space that has an infinite spanning set
%   \end{itemize}
% \end{definitionbox}
% \subsubsection*{Example of infinite dimensional vector space}
% \begin{itemize}
%   \item The vector space $\mathbb{R}[x]$ of all polynomials with real coefficients is infinite dimensional. To see this let $S$ be a finite subset of $\mathbb{R}[x]$ and let $x^k$ be the highest power of $x $ in $S$. Then $x^{k+1} \notin \langle S \rangle$ since $x^{k+1}$ cannot be expressed as a linear combination of the elements of $S$.
%   \item The set of $\mathbb{R}$ is infinite dimensional as a vector space over the field, $\mathbb{Q}$, of rational numbers.
% \end{itemize}
% \subsection{Linear independence}
% \begin{definitionbox}{}{}
%   Let $S \subseteq \mathbf{V}$ with at least two elements. \\[2ex]
%   Then $S$ is linearly independent if \textbf{no element of $S$ can be expressed as a linear combination of the other elements of $S$}. \\[2ex]
%   Equivalently, if no element of $S$ belongs to the span of the other elements of $S$.
% \end{definitionbox}
% It follows, a subset consisting of a single element is linearly independent if and only if that element is non-zero. The definition above takes a lot of work to check for large sets, the following definition is often more useful:
% \begin{definitionbox}{}{}
%   Let $S$ be a non-empty subset of $\mathbf{V}$.\\[2ex]
%   Then $S$ is \textbf{linearly independent} if the only linear combination of the elements of $S$ that equals zero is to take all the coefficients to be zero.
% \end{definitionbox}
% \subsubsection*{Equivalence of the two definitions}
% Let $S = \{v_1, \dots, v_k\}$ and suppose $v_1 \in \langle v_2, \dots v_k \rangle$. Then:
% $$
%   v_1 = \alpha_2 v_2 + \cdots + \alpha_k v_k \quad \Rightarrow \quad
%   0 = - v_1 + \alpha_2 v_2 + \cdots + \alpha_k v_k
% $$
% is an expression for the zero vector as a linear combination of elements of $S$, whose coefficients are not all zero. On the other hand suppose:
% $$
%   0 = c_1v_1 + c_2v_2 + \cdots + c_kv_k
% $$
% where not all $c_i = 0$ Then:
% $$
%   v_1 = -\frac{c_2}{c_1}v_2 - \cdots - \frac{c_k}{c_1}v_k \quad \Rightarrow \quad
%   v_1 \in \langle v_2, \dots, v_k \rangle
% $$
% \begin{examplebox}{}{}
%   In $\mathbb{R}^3$, let $S = \{[1,2,-1], [-2, 3, 2], [-3,8,3]\}$. Show that $S$ is linearly independent.
%   \\[2ex] \rule{\textwidth}{1px} \\
%   To determine if $S$ is linearly independent, we need to investigate whether the system of equations has solutions other than $(x,y,z) = (0,0,0)$:
%   $$ x \begin{bmatrix}
%       1 \\
%       2 \\
%       -1
%     \end{bmatrix}
%     + y \begin{bmatrix}
%       -2 \\
%       3  \\
%       2
%     \end{bmatrix}
%     + z \begin{bmatrix}
%       -3 \\
%       8  \\
%       3
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       0 \\
%       0 \\
%       0
%     \end{bmatrix}
%     \quad\Longrightarrow \quad
%     \begin{bmatrix}
%       1  & -2 & -3 & | & 0 \\
%       2  & 3  & 8  & | & 0 \\
%       -1 & 2  & 3  & | & 0
%     \end{bmatrix}
%   $$
%   Reducing it to its RREF we get:
%   $$
%     \begin{bmatrix}
%       1 & 0 & 1 & | & 0 \\
%       0 & 1 & 2 & | & 0 \\
%       0 & 0 & 0 & | & 0
%     \end{bmatrix}
%     \quad\Longrightarrow\quad
%     \begin{array}{l}
%       x  + t = 0 \\
%       y + 2t = 0 \\
%       z + t = 0
%     \end{array}
%     \quad\Longrightarrow\quad
%     (x,y,z) = (-t, -2t, t)
%   $$
%   Setting $t = 1$ gives:
%   $$
%     -1 \begin{bmatrix}
%       1 \\
%       2 \\
%       -1
%     \end{bmatrix}
%     -2 \begin{bmatrix}
%       -2 \\
%       3  \\
%       2
%     \end{bmatrix}
%     +1 \begin{bmatrix}
%       -3 \\
%       8  \\
%       3
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       0 \\
%       0 \\
%       0
%     \end{bmatrix}
%   $$
%   Hence, each of the three elements of $S$ is a linear combination of the other two. So $S$ is not linearly independent (we say it is linearly dependent).
% \end{examplebox}
% \subsection{Characterizations of Linear Independence}

% \begin{theorembox}
%   Let $S$ be a subset of a vector space $V$. The following are equivalent:
%   \begin{enumerate}
%     \item $S$ is linearly independent if and only if $S$ is a minimal spanning set of $\langle S \rangle$ -- no proper subset of $S$ spans $\langle S \rangle$.

%     \item $S$ is linearly independent if and only if every element of $\langle S \rangle$ has a unique expression as a linear combination of elements of $S$.

%     \item $S$ is linearly independent if and only if every element of $\langle S \rangle$ has unique coordinates with respect to the elements of $S$.
%   \end{enumerate}
% \end{theorembox}

% \begin{definitionbox}{Basis}{}
%   A \textbf{basis} of a vector space $V$ is a spanning set of $V$ that is linearly independent.
% \end{definitionbox}

% \begin{lemmabox}{}{}
%   If $S$ is a finite spanning set of a vector space $V$, then $S$ contains a basis of $V$.
% \end{lemmabox}

% \noindent \textbf{Proof:} If $S$ is not linearly independent, then some element $v_1 \in S$ is in the span of $S \setminus \{v_1\}$. Let $S_1 = S \setminus \{v_1\}$, which still spans $V$. Continue this process, removing elements that are linearly dependent on the remaining ones. Since $S$ is finite, this process terminates with a linearly independent spanning set of $V$.

% \begin{theorembox}{Steinitz Exchange Lemma}{}
%   Let $V$ be a vector space with a spanning set $S = \{v_1, \ldots, v_n\}$. Then any linearly independent subset $L$ of $V$ contains at most $n$ elements.
% \end{theorembox}
% \noindent \textbf{Proof:} Let $L = \{y_1, \ldots, y_k\}$ be linearly independent. The key idea is to replace elements of $S$ with elements of $L$ one by one, maintaining a spanning set:
% \begin{itemize}
%   \item Express $y_1$ as a linear combination of elements in $S$. At least one element, say $v_1$, must have a non-zero coefficient.
%   \item Replace $v_1$ with $y_1$ to get $S_1 = \{y_1, v_2, \ldots, v_n\}$, which still spans $V$.
%   \item Continue this process, replacing $v_i$ with $y_i$ at each step.
%   \item This can continue for at most $n$ steps, so $k \leq n$.
% \end{itemize}

% \begin{theorembox}
%   If $V$ is a finite-dimensional vector space, then every basis of $V$ has the same number of elements.
% \end{theorembox}

% \noindent \textbf{Proof:} Let $B_1$ and $B_2$ be bases of $V$. Since $B_1$ is linearly independent and $B_2$ spans $V$, we have $|B_1| \leq |B_2|$ by the Steinitz Exchange Lemma. Similarly, $|B_2| \leq |B_1|$. Therefore, $|B_1| = |B_2|$.

% \begin{definitionbox}{Dimension}{}
%   The dimension of a finite-dimensional vector space $V$, denoted $\dim V$, is the number of elements in any basis of $V$.
% \end{definitionbox}
% \pagebreak
% \subsection{Properties of Bases in Finite Dimensional Vector Spaces}
% Let $V$ be a vector space of dimension $n$ over a field $F$. Recall that a basis of $V$ has two key properties:
% \begin{itemize}
%   \item It is a linearly independent set in $V$
%   \item It spans $V$
% \end{itemize}

% A basis is therefore both a minimal spanning set and a maximal linearly independent set.

% \begin{theorembox}{Characterization of Bases by Cardinality}{}
%   For a finite-dimensional vector space $V$ with $\dim V = n$:
%   \begin{enumerate}
%     \item Every linearly independent subset with exactly $n$ elements is a basis.
%     \item Every spanning set with exactly $n$ elements is a basis.
%   \end{enumerate}
% \end{theorembox}

% \noindent \textbf{Proof of (1.):} Let $L = \{v_1, \ldots, v_n\}$ be a linearly independent subset of $V$.

% Suppose $L$ is not a spanning set. Then there exists some $v \in V$ with $v \notin \langle L \rangle$.

% This means the set $L' = \{v_1, \ldots, v_n, v\}$ would be linearly independent in $V$, contradicting Theorem 2.2.6 (which states that no linearly independent set can have more than $n$ elements).

% Therefore, $L$ must be a spanning set and thus a basis.\\[2ex]
% \noindent \textbf{Proof of (2.):}
% Let $S$ be a spanning set of $V$ with $n$ elements.

% If $S$ is not linearly independent, then $S$ contains a proper subset that spans $V$ but has fewer than $n$ elements, contradicting Theorem 2.2.6 (which implies all bases have exactly $n$ elements).

% Therefore, $S$ must be linearly independent and thus a basis.

% \begin{theorembox}{Basis Extension Theorem}{}
%   Any linearly independent subset $L$ of $V$ can be extended to a basis of $V$.
% \end{theorembox}

% \begin{proof}
%   Let $L = \{v_1, \ldots, v_k\}$. Since $L$ is linearly independent, $k \leq n$ by Theorem 2.2.6.

%   If $k = n$, then $L$ is already a basis by part (1) above.

%   If $k < n$, then $L$ does not span $V$, so there exists $v_{k+1} \in V$ with $v_{k+1} \notin \langle L \rangle$. Then $\{v_1, \ldots, v_k, v_{k+1}\}$ is linearly independent.

%   We can continue this process, adding elements outside the existing span until we reach $n$ elements, which will form a basis of $V$.
% \end{proof}

% \begin{theorembox}{Isomorphism with $F^n$}{}
%   Every $n$-dimensional vector space $V$ over $F$ is isomorphic to the standard vector space $F^n$.
% \end{theorembox}

% \noindent\textbf{Proof:} Let $B = \{v_1, \ldots, v_n\}$ be a basis of $V$ over $F$.

% For any $v \in V$, there exists a unique expression $v = a_1v_1 + \cdots + a_nv_n$.

% The mapping $v \mapsto \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix}$ defines a bijective correspondence between $V$ and $F^n$ that preserves vector space operations.

% Different bases of $V$ correspond to different isomorphisms with $F^n$.


% \begin{definitionbox}{Standard Basis}{}
%   The standard basis of $F^n$ is $\{e_1, \ldots, e_n\}$, where $e_i$ has 1 in position $i$ and 0 in all other positions.
% \end{definitionbox}
\end{document}