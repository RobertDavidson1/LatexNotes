\documentclass[a4paper, 9pt]{extarticle}
\input{../../preamble.sty}
\input{../../macros.sty}

\title{
\textbf{MA283: Linear Algebra} \\ 
}

\usepackage{geometry}
 \geometry{
 a4paper,
 bottom=22mm,
 }

\author{
  70\% Exam\\
30\% Continuous Assessment (Homework) \\
10\% Optional Project (Bonus)\\ [2ex]
Robert Davidson
}     
\date{}       % Optional: Add date if desired

\begin{document}

\maketitle
\pagebreak

\tableofcontents
\pagebreak
\section{Systems of linear equations}
\subsection{Linear equations and Solution Sets}
A linear equation in the variables $x$ and
$y$ is an equation of the form
\begin{equation*}
  2x + y = 3
\end{equation*}
If we replace $x$ and $y$ with some numbers, the statement \textbf{becomes true or false}.

\begin{definitionbox}{Solution to a linear equation}{}
  A pair, $(x_0, y_0) \in \mathbb{R}$, is a solution to an linear equation if setting $x = x_0$ and $y = y_0$ \textbf{makes the equation true.}
\end{definitionbox}

\begin{definitionbox}{Solution set}{}
  The \textbf{solution set} is the set of all solutions to a linear equation.
  $$a_1X_1 + a_2X_2 + \ldots + a_nX_n = b \quad \text{where} \; a_i, b \in \mathbb{R}$$
  is an \textbf{affine hyperplane} in $\mathbb{R}^n$; geometrically resembles a copy of $\mathbb{R}^{n-1}$ inside $\mathbb{R}^n$.
\end{definitionbox}
\subsection{Elementary Row Operations}
To solve a system of linear equations we associate an \textbf{augmented matrix} to the system of equations. For example:
$$
  \begin{array}
    {ccccccc}x & + & 2y & - & z  & = & 5 \\
    3x         & + & y  & - & 2z & = & 9 \\
    -x         & + & 4y & + & 2z & = & 0
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1  & 2 & -1 & | & 5 \\
    3  & 1 & -2 & | & 9 \\
    -1 & 4 & 2  & | & 0
  \end{bmatrix}
$$
To solve, we can perform the following \textbf{Elementary Row Operations (EROs)}:
\begin{enumerate}
  \item Multiply a row by a non-zero constant.
  \item Add a multiple of one row to another row.
  \item Swap two rows.
\end{enumerate}
The goal of these operations is to transform the augmented matrix into \textbf{row echelon form} (REF) or \textbf{reduced row echelon form} (RREF).

\subsubsection{REF and Strategy}
\begin{minipage}{0.8\textwidth}
  We say a matrix is in \textbf{row echelon form} (REF) if:
  \begin{itemize}
    \item The first non zero entry in each row is a 1 (called the \textbf{leading 1}).
    \item If a column has a leading 1, then all entries below it are 0.
    \item The leading 1 in each row is to the right of the leading 1 in the previous row.
    \item All rows of 0s are at the bottom of the matrix.
  \end{itemize}
\end{minipage}
\begin{minipage}{0.2\textwidth}
  \begin{center}
    $$
      \begin{bmatrix}
        1 & 2 & -1 & | & 3  \\
        0 & 1 & 2  & | & -1 \\
        0 & 0 & 1  & | & -1
      \end{bmatrix}
    $$
    \emph{Example of REF}
  \end{center}


\end{minipage} \\[2ex]
We have produced a new system of equations. This is easily solved by back substitution.

\begin{conceptbox}{Stategy for Obtaining REF}{}
  \begin{itemize}
    \item Get a 1 as the top left entry
    \item Use this 1 to clear the entries below it
    \item Move to the next column and repeat
    \item Continue until all leading 1s are in place
    \item Use back substitution to solve the system
  \end{itemize}
\end{conceptbox}
\subsubsection{Row Reduced Echelon Form}
\begin{minipage}{0.8\textwidth}
  A matrix is in \textbf{reduced row echelon form} (RREF) if:
  \begin{itemize}
    \item It is in REF
    \item The leading 1 in each row is the only non-zero entry in its column.
  \end{itemize}
\end{minipage}
\begin{minipage}{0.2\textwidth}
  \begin{center}
    $$
      \begin{bmatrix}
        1 & 0 & 0 & | & 2  \\
        0 & 1 & 0 & | & -1 \\
        0 & 0 & 1 & | & -1
      \end{bmatrix}
    $$
    \emph{Example of RREF}
  \end{center}
\end{minipage}

\subsection{Leading variables and free variables}
We'll start by an example:
$$
  \begin{array}{ccccccccc}
    x_1  & - & x_2  & - & x_3  & + & 2x_4 & = & 0 \\
    2x_1 & + & x_2  & - & x_3  & + & 2x_4 & = & 8 \\
    x_1  & - & 3x_2 & + & 2x_3 & + & 7x_4 & = & 2
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1 & -1 & -1 & 2 & | & 0 \\
    2 & 1  & -1 & 2 & | & 8 \\
    1 & -3 & 2  & 7 & | & 2
  \end{bmatrix}
$$
Solving this system of equations, we get:
$$
  \text{RREF:}\quad
  \begin{bmatrix}
    1 & 0 & 0 & 2  & | & 4 \\
    0 & 1 & 0 & -1 & | & 2 \\
    0 & 0 & 0 & 1  & | & 2
  \end{bmatrix}
  \quad \Rightarrow \quad
  \begin{array}{ccccc}
    x_1 & + & 2x_4 & = & 4 \quad \\
    x_2 & - & x_4  & = & 2 \quad \\
    x_3 & + & x_4  & = & 2 \quad
  \end{array}
  \quad \Rightarrow \quad
  \begin{array}{ccc}
    x_1 & = & 4 - 2x_4 \\
    x_2 & = & 2 + x_4  \\
    x_3 & = & 2 - x_4
  \end{array}
$$
This RREF tells us how the \textbf{leading variables} ($x_1, x_2, x_3$) depend on the \textbf{free variable} ($x_4$). The free variable can take any value in $\mathbb{R}$. We write the solution set as:
$$x_1 = 4 - 2t, \quad x_2 = 2 + t, \quad x_3 = 2 - t, \quad x_4 = t \quad \text{where} \; t \in \mathbb{R}$$
$$(x_1, x_2, x_3, x_4) = (4-2t, 2+t, 2-t, t); \quad t \in \mathbb{R}$$
\begin{definitionbox}{Leading and Free Variables}{}
  \begin{itemize}
    \item  \textbf{Leading variable} : A variable whose columns in the RREF contain a leading 1
    \item \textbf{Free variable} : A variable whose columns in the RREF do not contain a leading 1
  \end{itemize}
\end{definitionbox}

\subsection{Consistent and Inconsistent Systems}
Consider the following system of equations:
$$
  \begin{array}{ccccccc}
    3x & + & 2y & - & 5z & = & 4 \\
    x  & + & y  & - & 2z & = & 1 \\
    5x & + & 3y & - & 8z & = & 6
  \end{array}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    3 & 2 & -5 & | & 4 \\
    1 & 1 & -2 & | & 1 \\
    5 & 3 & -8 & | & 6
  \end{bmatrix}
  \quad \Rightarrow \quad
  \begin{bmatrix}
    1 & 1 & -2 & | & 1 \\
    0 & 1 & -1 & | & 1 \\
    0 & 0 & 0  & | & 1
  \end{bmatrix} \quad \text{(REF)}
$$
We can see the last row of the REF is:
$$
  0x + 0y + 0z = 1
$$
This equation clearly has no solution, and hence the system has no solutions. We say the system is \textbf{inconsistent}. Alternatively, we say the system is \textbf{consistent} if it has at least one solution.
\subsection{Possible Outcomes when solving a system of equations}
\begin{itemize}
  \item The system may be \textbf{inconsistent} (no solutions) - i.e:
        $$ [0 \; 0 \; \dots \; 0 \; | \; a] \quad a \neq 0$$
  \item The system may be \textbf{consistent} which occurs if:
        \begin{itemize}
          \item \textbf{Unique Solutions} each column (aside from the rightmost) contains a single leading 1. - i.e:
                $$
                  \begin{bmatrix}
                    1 & 0 & 0 & | & 4  \\
                    0 & 1 & 0 & | & 3  \\
                    0 & 0 & 1 & | & -2
                  \end{bmatrix}
                $$

          \item \textbf{Infinitely many solutions} at least one variable does not appear as a leading 1 in any row, making it a free variable - i.e:
                $$
                  \begin{bmatrix}
                    1 & 2 & -1 & | & 3  \\
                    0 & 0 & 1  & | & -2 \\
                    0 & 0 & 0  & | & 0
                  \end{bmatrix}
                $$
        \end{itemize}
\end{itemize}

\subsection{Elementary Row Operations as Matrix Transformations}
Elementary row operations may be interpreted as \textbf{matrix multiplication}. To see this, first we introduce the \textbf{Identity matrix:}
\vspace{-2ex}
$$I_3 = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix}$$
The $I_m$ Identity matrix is an $m \times m$ matrix with 1s on the diagonal and 0s elsewhere. We also introduce the $E_{i,j}$ matrix which has $1$ in the $(i,j)$ position and $0$s elsewhere. For example:
$$
  E_{1,2} = \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
$$
Then:
\vspace{-2ex}
$$I_3 + 4E_{1,2} = \begin{bmatrix}
    1 & 4 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix}$$
Performing a row operation on $A$ is the same as multiplying $A$ by an appropriate matrix $E$ on the left.
These matrices are called \textbf{elementary matrices}. They are \textbf{always invertible,} and \textbf{their inverses are also elementary matrices}. The statement:
$$
  "\emph{every matrix can be reduced to RREF through EROs}"$$
is equivalent to saying that
\begin{align*}
  "\emph{for every matrix $A$ with $m$ rows, there exists a $m \times m$ matrix $B$} \\
  \emph{which is a product of elementary matrices such that $BA$ is in RREF.}"
\end{align*}
\vspace{-8ex}
\subsubsection{Multiplying a Row by a Non-Zero Scalar}
When multiplying row $i$ of matrix $A$ by a scalar $\alpha \neq 0$, we can use the matrix:
$$I_m + (\alpha - 1)E_{i,i}$$
This works because it modifies only the $(i,i)$ entry of the identity matrix to be $\alpha$ while keeping all other entries unchanged. When multiplied with $A$, it scales row $i$ by $\alpha$ and leaves all other rows intact.\\
\textbf{Example:} If $\alpha = 5$ and $i = 2$, then:
\vspace{-2ex}
$$
  I_3 + 4E_{2,2} = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 5 & 0 \\
    0 & 0 & 1
  \end{bmatrix}\
  \quad
  A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad \quad\quad
  (I_3 + 4E_{2,2})A = \begin{bmatrix}
    1  & 2  & 3  \\
    20 & 25 & 30 \\
    7  & 8  & 9
  \end{bmatrix}$$
\vspace{-4ex}
\subsubsection{Switching Two Rows}
To swap rows $i$ and $k$, we use:
$$S = I_m + E_{i,k} + E_{k,i} - E_{i,i} - E_{k,k}$$
This works by:
\begin{itemize}
  \item Removing the 1's at positions $(i,i)$ and $(k,k)$ from the identity matrix
  \item Adding 1's at positions $(i,k)$ and $(k,i)$
\end{itemize}
\vspace{-1ex}
\textbf{Example:} Swapping rows 1 and 3:
\vspace{-2ex}
$$  S = \begin{bmatrix}
    0 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 0
  \end{bmatrix}
  \quad
  A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad \quad\quad
  SA = \begin{bmatrix}
    7 & 8 & 9 \\
    4 & 5 & 6 \\
    1 & 2 & 3
  \end{bmatrix}$$
\vspace{-5ex}
\subsubsection{Adding a Multiple of One Row to Another}
To replace row $k$ with row $k + \alpha \times$ row $i$, use:
$$I_m + \alpha E_{k,i}$$
This adds $\alpha$ times row $i$ to row $k$ while leaving all other rows unchanged because:
\begin{itemize}
  \item For any row $j \neq k$, the corresponding row in this matrix is just the standard basis row
  \item Row $k$ becomes the sum of the standard basis row $k$ plus $\alpha$ times the standard basis row $i$
\end{itemize}
\textbf{Example:} Adding 3 times row 1 to row 2:
\vspace{-1ex}
$$ I_3 + 3E_{2,1} = \begin{bmatrix}
    1 & 0 & 0 \\
    3 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix} \quad A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{bmatrix}
  \quad\quad\quad
  (I_3 + 3E_{2,1})A = \begin{bmatrix}
    1 & 2  & 3  \\
    7 & 11 & 15 \\
    7 & 8  & 9
  \end{bmatrix}$$

\begin{examplebox}{}{}
  Write the inverse of an elementary matrix and show it is an elementary matrix.
  \\ \rule{\textwidth}{1px} \\
  \textbf{Multiplying a row by a nonzero scalar:}
  \begin{itemize}
    \item \textbf{Operation:} Multiply row $i$ by $\alpha \neq 0$.
    \item \textbf{Elementary Matrix:} $E = I_m + (\alpha - 1)E_{i,i}$
    \item \textbf{Inverse:} To reverse the operation, multiply row $i$ by $1\backslash\alpha$. Hence, the inverse is
          $$
            E^{-1} = I_m + \left(\frac{1}{\alpha} - 1\right)E_{i,i}  =\; I_m + \frac{1-\alpha}{\alpha}\,E_{i,i}.
          $$
  \end{itemize}
  \textbf{Swapping two rows:}
  \begin{itemize}
    \item \textbf{Operation:} Swap rows $i$ and $k$.
    \item \textbf{Elementary Matrix:} $S = I_m - E_{i,i} - E_{k,k} + E_{i,k} + E_{k,i}$
    \item \textbf{Inverse:} Since swapping the same two rows twice returns them to their original positions,
          $$
            S^{-1} = S.
          $$
  \end{itemize}
  \textbf{Adding a multiple of one row to another:}
  \begin{itemize}
    \item \textbf{Operation:} Add $\alpha$ times row $i$ to row $k$.
    \item \textbf{Elementary Matrix:} $E = I_m + \alpha\,E_{k,i}$
    \item \textbf{Inverse:} To undo the operation, subtract $\alpha$ times row $i$ from row $k$. Therefore,
          $$
            E^{-1} = I_m - \alpha\,E_{k,i}.
          $$
  \end{itemize}
\end{examplebox}
\begin{examplebox}{}{}
  Prove that every invertible matrix in $M_n(\mathbb{R})$ is a product of elementary matrices.
  \\ \rule{\textwidth}{1px} \\
  Let $A$  be an invertible matrix in $M_n(\mathbb{R})$. Since $A$ is invertible, we can use Gaussian elimination to transform $A$ into the identity matrix $I_n$. \\[2ex]
  Let $E_1, E_2, \ldots, E_k$ be the elementary matrices corresponding to the row operations used in the elimination process. Then, we have:
  \begin{align*}
    \text{Multiplying a row by a scalar:}           & \quad I_n + (\alpha - 1)E_{i,i}                   \\
    \text{Swapping two rows:}                       & \quad I_n + E_{i,k} + E_{k,i} - E_{i,i} - E_{k,k} \\
    \text{Adding a multiple of one row to another:} & \quad I_n + \alpha\,E_{k,i}
  \end{align*}
  Applying these in sequence to $A$ gives:
  $$E_k \cdots E_2 E_1 A = I_n$$
  Since $E_k \cdots E_2 E_1 = I_n$, we can multiply both sides by $(E_k \cdots E_2 E_1)^{-1}$ on the left to obtain:
  $$A = (E_k \cdots E_2 E_1)^{-1} I_n = (E_k \cdots E_2 E_1)^{-1}$$
  Using the property:
  $$(E_k \cdots E_2 E_1)^{-1} = E_1^{-1} E_2^{-1} \cdots E_k^{-1},$$
  we can express $A$ as a product of elementary matrices:
  $$A = E_1^{-1} E_2^{-1} \cdots E_k^{-1}$$
  Since each $E_i$ is an elementary matrix, its inverse is also an elementary matrix. Therefore, $A$ can be expressed as a product of elementary matrices.
\end{examplebox}
\pagebreak
\subsection{EROs and Inverses}
Elementary Row Operations can be used to find the inverse of a square matrix.  Consider a square matrix $A \in M_n(\mathbb{F})$ (that is, an $n \times n$ matrix over a field $\mathbb{F}$). If $A$ is invertible, let
$$
  A^{-1}
  =
  \begin{bmatrix}
    |            & |            &        & |            \\
    \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \\
    |            & |            &        & |
  \end{bmatrix}
$$
be its inverse, where each $\mathbf{v}_i$ is the $i$th column of $A^{-1}$. By definition of the matrix inverse, we have
$$
  A \, A^{-1}
  =
  A \begin{bmatrix}
    \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n
  \end{bmatrix}
  =
  \begin{bmatrix}
    A \mathbf{v}_1 & A \mathbf{v}_2 & \cdots & A \mathbf{v}_n
  \end{bmatrix}
  =
  I_n,
$$
the $n \times n$ identity matrix. This implies that
$$
  A \mathbf{v}_i = \mathbf{e}_i, \quad \text{for each } i=1, 2, \ldots, n,
$$
where $\mathbf{e}_i$ is the $i$th column of $I_n$ (which has a 1 in the $i$th row and 0 everywhere else). In other words, each column $\mathbf{v}_i$ of $A^{-1}$ is the unique solution to the linear system
$$
  A \mathbf{v}_i = \mathbf{e}_i.
$$
To find $A^{-1}$ effectively, we form the augmented matrix $[A \mid I_n]$ and apply EROs to trasnform $A$ into $I_n$. When this is achieved, the augmented portion becomes $A^{-1}$. Thus, we have
$$
  \text{RREF}\bigl([\,A \mid I_n\,]\bigr)
  =
  \bigl[\, I_n \mid A^{-1} \bigr].
$$
\begin{examplebox}{}{}
  Find $A^{-1}$ if $A = \begin{bmatrix}
      3 & 4 & -1 \\
      1 & 0 & 3  \\
      2 & 5 & -4
    \end{bmatrix}$.
  \\[2ex] \rule{\textwidth}{1px} \\
  We form a $3 \times 6$ matrix $A' = [A \mid I_3]$:
  $$
    A' = \begin{bmatrix}
      3 & 4 & -1 & | & 1 & 0 & 0 \\
      1 & 0 & 3  & | & 0 & 1 & 0 \\
      2 & 5 & -4 & | & 0 & 0 & 1
    \end{bmatrix}
  $$
  We apply the following EROs to $A'$:
  \begin{itemize}
    \item $R_1 \leftrightarrow R_2$
    \item $R_2 \rightarrow R_2 - 3R_1$
    \item $R_3 \rightarrow R_3 - 2R_1$
    \item $R_3 \rightarrow R_3 + R-2$
    \item $R_3 \leftrightarrow R_2$
    \item $R_3 \rightarrow R_3 - 4R_2$
    \item $R_3 \times (-\frac{1}{10})$
    \item $R_1 \rightarrow R_1 - 3R_3$
  \end{itemize}
  To obtain:
  $$
    \begin{bmatrix}
      1 & 0 & 0 & | & \frac{3}{2}  & -\frac{11}{10} & -\frac{6}{5} \\
      0 & 1 & 0 & | & -1           & 1              & 1            \\
      0 & 0 & 1 & | & -\frac{1}{2} & \frac{7}{10}   & \frac{2}{5}
    \end{bmatrix}
  $$
  That is:
  $$A^{-1} = \begin{bmatrix}
      \frac{3}{2}  & -\frac{11}{10} & -\frac{6}{5} \\
      -1           & 1              & 1            \\
      -\frac{1}{2} & \frac{7}{10}   & \frac{2}{5}
    \end{bmatrix}$$
  It is easily checked that $AA^{-1} = I_3$.
\end{examplebox}
\pagebreak
\subsection{Review of Matrix Algebra}
\subsubsection{Matrices Over a Field}
\begin{itemize}
  \item An $m \times n$ matrix over a field $\mathbb{F}$ is an array of $m$ rows and $n$ columns of elements from $\mathbb{F}$.
  \item When $m = n$, we write $M_n(\mathbb{F})$, otherwise we write $M_{m \times n}(\mathbb{F})$.
\end{itemize}
\subsubsection{Addidion and Scalar Multiplication}
\begin{itemize}
  \item Two matrices of the same size $m \times n$ can be added entrywise
  \item The $m \times n$ matrix has all entries equal to zero and acts as the additive identity (adding it to any matrix does not change the matrix)
  \item Multiplying a matrix by a scalar means multiplying each entry by that scalar
  \item The set of all $m \times n$ matrices over $\mathbb{F}$ is a vector space over $\mathbb{F}$
\end{itemize}
\subsubsection{Linear Combinations}
\begin{itemize}
  \item A linear combination of vectors $v_1, v_2, \ldots, v_k$ in a vector space $V$ with coefficients $a_1, a_2, \ldots, a_k \in \mathbb{F}$ is defined as:
        $$ a_1v_1 + a_2v_2 + \ldots + a_kv_k$$
  \item In particular, matrices themselves can be combined linearly, (e.g. $2A - 3B$)
\end{itemize}
\subsubsection{Row and Column Vectors}
\begin{itemize}
  \item A column vector is a matrix with one column
  \item A row vector is a matrix with one row
\end{itemize}
\subsubsection{Matrix-Vector Multiplication}
\begin{itemize}
  \item If $A$ is $m \times n$ matrix and $v$ is an $n$-entry column vector, the product $Av$ is defined by taking a linear combination of the columns of $A$ with the entries of $v$ as coefficients.
  \item The result $Av$ is an $m$-entry column vector.
  \item For a row vector $u$ with $m$ entries, and an $m \times n$ matrix $A$m the product $uA$ a row vector in $\mathbb{F}^n$ formed by the linear combination of the rows of $A$ with the entries of $u$ as coefficients.
\end{itemize}
\subsubsection{Matrix-Matrix Multiplication}
\begin{itemize}
  \item If $A$ is a $m \times p$ and $B$ is a $p \times n$ matrix, the product $AB$ is defined only when the inner dimensions match $(p)$
  \item To find each column of $AB$, multiply $A$ with the corresponding column vector of $B$.
  \item In entrywise form:
        $$
          (AB)_{ij} = A_{i,1}B_{1,j} + A_{i,2}B_{2,j} + \ldots + A_{i,p}B_{p,j}
        $$
\end{itemize}
\end{document}